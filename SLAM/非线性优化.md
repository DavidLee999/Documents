# 非线性优化

在SLAM 中，由于噪声的影响，观测方程和运动方程都只能近似成立。但在整个过程中，会有很多多余观测，即一个特征会被机器人在不同位置、不同时间进行观测，每次也都会观测到多个特征，这就形成了约束。利用这些约束，就可以对数据进行优化。

## 状态估计问题

###问题的引出

一个经典的SLAM 模型由一个运动方程和一个观测方程组成：
$$
\begin{cases}
\mathbf{x}_k=f(\mathbf{x}_{k-1},\mathbf{u}_k)+\omega_k \\
\mathbf{z}_{k,j}=h(\mathbf{y}_j,\mathbf{x}_k) + v_{k,j}
\end{cases}
$$
注意，这里上一时刻的位姿$\mathbf{x}_{k-1}$ 是已知的；第j 个路标的观测值$\mathbf{z}_j$ 和机器人运动测量值$\mathbf{u}_k$ 是已知的；最后两项是噪声。而未知值是此刻的位姿$\mathbf{x}_{k}$ 和路标的世界坐标$\mathbf{y}_{j}$。

通过上一讲我们知道，相机的位姿可以用$\mathbf{T}_k$ 或者$\exp(\xi_k^\wedge)$ 来表达，设相机矩阵为**K**，观测值**z** 为对应路标的像素坐标，观测方程就可以写为：
$$
s\mathbf{z}_{k,j}=\mathbf{K}\exp(\xi^\wedge)\mathbf{y}_j
$$
s 为路标$\mathbf{z}_j$ 在对应相机坐标系下的深度。这里我们使用了齐次坐标并包含有一次齐次到非齐次的坐标转换。作为引出这里先不展开，后面会详细解释。

如前所述，由于噪声的存在，这个式子只能在数学上成立。实际中必须在后面加上一个噪声项v。通常都假设两个方程的噪声都满足**零均值的高斯分布**：
$$
\omega_k \sim N(0, \mathbf{R}_k),\ \mathbf{v}_k\sim N(0,\mathbf{Q}_{k, j})
$$
在这些噪声的影响下，我们希望通过带噪声的数据**z** 和**u** 来推断位姿**x** 和地图**y**，这就构成了一个状态估计问题。常用的方法有滤波器和非线性优化。滤波器会在后面结束，这里重点介绍非线性优化的方法。

### 最大后验和最大似然

我们从概率学的角度检视下这个状态估计问题。在非线性优化中，所有待优化的变量都放在一个*状态变量*中：
$$
\mathbf{x}=\{\mathbf{x}_1,\ldots\mathbf{x}_N,\mathbf{y}_1,\ldots\mathbf{y}_M\}
$$
表示包含了N 个时刻的相机位姿和M 个路标。而对机器人状态的估计，就是已知输入数据**u** 和观测数据**z** 的条件下，求状态**x** 的条件概率分布：
$$
P(\mathbf{x}|\mathbf{z},\mathbf{u})
$$
如果我们并没有测量运动的传感器，如IMU等，只考虑视觉部分，则上式就变成了$P(\mathbf{x}|\mathbf{z})$。利用贝叶斯法则可得：
$$
P(\mathbf{x}|\mathbf{z})=\frac{P(\mathbf{z}|\mathbf{x})P(\mathbf{x})}{P(\mathbf{z})}\propto P(\mathbf{z}|\mathbf{x})P(\mathbf{x})
$$
等式的左侧称为**后验概率**，P(**x**) 和P(**z**) 是先验概率，P(**z**|**x**) 称为似然。先验概率一般通过观测频率而得到。而P(**z**) 和状态变量**x** 无关（一般也为固定值），所以略去。**直接去求解后验分布一般比较困难，但是求一个状态最优估计，使得在该状态下后验概率最大化 (MAP)，则是可行的**。
$$
\mathbf{x}^*_{MAP}=\arg \max P(\mathbf{x}|\mathbf{z})=\arg \max P(\mathbf{z}|\mathbf{x})P(\mathbf{x})
$$
从上式可以看到，求解最大后验概率相当于**最大化似然和先验概率的乘积**。更进一步地，我们可以说：呃，其实我也不太知道机器人现在大概在什么地方哎。此时连先验概率P(**x**) 也可以忽略（P(**x**) 一般可由之前的状态估计得到，在这一步中一般也是一个固定值，忽略了也没影响）。那么，问题就变成了**求解x 的最大似然估计 (MLE)**：
$$
x^*_{MLE}=\arg\max P(\mathbf{z}|\mathbf{x})
$$
从概率的角度来说，上式就是*在事件**x** 已经发生的情况下，事件**z** 发生的概率*。代入到SLAM 的背景中，似然就指*在现有的位姿下，能够产生什么样的观测数据*。前面说过，**z** 才是已知量，所以上式应该理解成*在什么样的位姿下，可以产生现在这些观测数据*。

## 最小二乘

### 引出

回顾上一节的观测模型：
$$
\mathbf{z}_{k,j}=h(\mathbf{y}_j,\mathbf{x}_k) + \mathbf{v}_{k,j}
$$
我们假设了噪声项满足零均值的高斯分布：
$$
\mathbf{v}_k\sim N(0,\mathbf{Q}_{k, j})
$$
要注意的是，式(9) 的前半部分并不包含任何概率项，是噪声项**v** 引入了概率（高斯分布）。所以，式(9) 相当于把零均值的高斯分布平移了一段$h(\mathbf{y}_j,\mathbf{x}_k)$。所以，观测数据的条件概率为：
$$
P(\mathbf{z}_{j,k}|\mathbf{x}_k,\mathbf{y}_j) = N(h(\mathbf{y}_j,\mathbf{x}_k), \mathbf{Q}_{k, j}))
$$
即为均值为$h(\mathbf{y}_j,\mathbf{x}_k)$ 的高斯分布。为了计算使它最大化的$\mathbf{x}_k,\mathbf{y}_j$，可以使用**最小化负对数**的方法。

考虑任意维的高斯分布$\mathbf{x}_k\sim N(\mu,\mathbf{\Sigma})$ (这里**x** 不表示位姿)，它的概率密度函数为：
$$
P(\mathbf{x})=\frac{1}{\sqrt{(2\pi)^N\det(\mathbf{\Sigma})}}\exp(-\frac{1}{2}(\mathbf{x}-\mu)^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\mu))
$$
取其负对数可得：
$$
-\ln(P(\mathbf{x}))=\frac{1}{2}\ln((2\pi)^N\det(\mathbf{\Sigma}))+\frac{1}{2}(\mathbf{x}-\mu)^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\mu)
$$
求原分布的最大化显然就是求其负对数的**最小化**。而上式第一项和**x** 无关，所以只需要最小化右侧的二次型项。将之代入SLAM 的观测模型可得：
$$
\mathbf(x)^*_{MLE}=\arg\min (\mathbf{z}_{k,j}-h(\mathbf{y}_j,\mathbf{x}_k))^T\mathbf{\Sigma}^{-1}(\mathbf{z}_{k,j}-h(\mathbf{y}_j,\mathbf{x}_k))
$$
该式就相当于**最小化噪声项的平方**。

对运动方程和观测方程，定义数据和估计值之间的误差为：
$$
\begin{align}
\mathbf{e}_{v, k}& =\mathbf{x}_k-f(\mathbf{x}_{k-1},\mathbf{u}_k) \\
\mathbf{e}_{y,j,k}& = \mathbf{z}_{k,j}-h(\mathbf{x}_k, \mathbf{y}_j)
\end{align}
$$
再求该误差项的平方和就可得：
$$
J(\mathbf{x})=\sum_k\mathbf{e}^T_{v,k}\mathbf{R}^{-1}_k\mathbf{e}_{v,k}+\sum_k\sum_j\mathbf{e}^T_{y,k,j}\mathbf{Q}^{-1}_{k,j}\mathbf{e}_{y,k,j}
$$
这样就得到了一个总体意义下的（非线性）最小二乘问题，它的最优解等价于状态的最大似然估计。对它的求解是一个典型的非线性优化的过程。

### 非线性最小二乘

式(17) 比较复杂，作为基础理论的介绍，我们先考虑一个类似的、简单的非线性最小二乘问题。目标函数为：
$$
\min_x \frac{1}{2}||f(x)||^2_2
$$
这里自变量$\mathbf{x}\in R^n$ 是个n 维向量，f 是任意非线性函数，假设它有m 维。下面讨论下如何求解这样一个优化问题。

如果f 在数学上是个很简单函数，那么很自然地，这个问题可以用**解析形式**来求。令目标函数的导数为0，然后求解**x** 的最优值。
$$
\frac{df}{d\mathbf{x}}=0
$$
解这个方程，就可以得到导数为零处的极值，它们可能是极大、极小或者鞍部的点，逐个比较其大小就可以得到结果了。

然后，我想讨论下法方程的解法。后面的一些讨论和公式的推导都和下面类似，这里先列出，后面就不在赘述了。如果我们没有目标函数的具体形式，只有数值。假设目标函数是个线性方程：$\min_x\frac{1}{2}||\mathbf{y}-\mathbf{A}\mathbf{x}||^2_2$， **A** 是一个m*n  的矩阵，表示n 维到m 维空间的映射。这里的1/2 是为了后面求导后消去系数，对方程的解没有影响。该式可以写成：
$$
\min_x \frac{1}{2}(\mathbf{y}-\mathbf{A}\mathbf{x})^T(\mathbf{y}-\mathbf{A}\mathbf{x})
$$
展开可得：
$$
\begin{align}
& \frac{1}{2}(\mathbf{y}-\mathbf{A}\mathbf{x})^T(\mathbf{y}-\mathbf{A}\mathbf{x})\\ 
= & \frac{1}{2}(\mathbf{y}^T\mathbf{y}-\mathbf{y}^T\mathbf{A}\mathbf{x} - \mathbf{x}^T\mathbf{A}^T\mathbf{y}+ \mathbf{x}^T\mathbf{A}^T\mathbf{A}\mathbf{x}) \\
= & \frac{1}{2}(\mathbf{y}^T\mathbf{y}-2\mathbf{y}^T\mathbf{A}\mathbf{x}+ \mathbf{x}^T\mathbf{A}^T\mathbf{A}\mathbf{x})\\
\end{align}
$$
将上式对**x** 求导并等于0 可得：
$$
-\mathbf{A}^T\mathbf{y}+ \mathbf{A}^T\mathbf{A}\mathbf{x}=0
$$
即可解得：
$$
\mathbf{x} = (\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{y}
$$
最后，虽然在上一讲中我们引入了李代数和它的导数形式，但这不意味着我们就可以顺利地用它们来求出解析解。其次，从前面的介绍我们看到问题是非线性的，而且数据量可能很大，也不适合上面这个推导和用法方程来解。类比机器学习中的梯度下降法，我们可以采用**迭代**的方法。从一个**初始值**出发，不断地更新当前的优化变量使得目标函数下降。其步骤如下：

> 1. 给定某个初始值$\mathbf{x}_0$；
> 2. 对于第k 次迭代，寻找一个增量$\Delta \mathbf{x}_k$，使得$||f(\mathbf{x}_k+\Delta\mathbf{x}_k| |$ 达到极小值；
> 3. 若$\Delta \mathbf{x}_k$ 足够下则停止迭代；
> 4. 否则，令$\mathbf{x}_{k+1}=\mathbf{x}_k + \Delta \mathbf{x}_k$.

这就让求解导数为零的问题变成了一个不断寻找梯度并下降的过程。我们只要找到迭代点的梯度方向即可，而不必寻找全局导数为0 的情况。

接下来的问题就是，如何确定增量$\Delta \mathbf{x}_k$？

### 一阶和二阶梯度法

熟悉机器学习的话可以很快就想到对目标函数进行求导即可。这里我们还是从头说起，并引出其他方法。

最直观地，就是将目标函数在**x** 附近进行**泰勒展开**：
$$
||f(\mathbf{x}+\Delta \mathbf{x})||^2_2\approx||f(\mathbf{x})||^2_2+\mathbf{J}(\mathbf{x})\Delta \mathbf{x}+\frac{1}{2}\Delta \mathbf{x}^T\mathbf{H}\Delta\mathbf{x}
$$
这里，**J** 是$||f(\mathbf{x})||^2_2$ 关于**x** 的导数，称为**雅可比矩阵**；**H** 是二阶导数，称为**海塞矩阵**。**注意**：泰勒展开需要给定一个初始值，所以这里$||f(\mathbf{x})||^2_2$ 是计算出的已知值了。

如果我们只保留一阶梯度，将上式忽略后面的二次项，对$\Delta \mathbf{x}$ 求导就可以得到增量的解：
$$
\Delta \mathbf{x}^*=-\mathbf{J}^T(\mathbf{x})
$$
上式的意义也很直观：沿着与梯度方向相反的方向前进即可。通常我们会在该方向上加上一个步长$\lambda$，求得最快的下降方式。该方法被称为**最速下降法**。

如果保留二阶梯度，那么增量方程就变成：
$$
\Delta \mathbf{x}^*=\arg\min||f(\mathbf{x}||^2_2+\mathbf{J}(\mathbf{x})\Delta \mathbf{x}+\frac{1}{2}\Delta \mathbf{x}^T\mathbf{H}\Delta\mathbf{x}
$$
将等式右边对$\Delta \mathbf{x}$ 求导并等于0 即可得：
$$
\mathbf{H}\Delta \mathbf{x}=-\mathbf{J}^T
$$
该方法又称为牛顿法。但这个方法的问题在于需要计算海塞矩阵**H**。基于这节的讨论，下面介绍两种在实践中常用的方法：高斯牛顿法和列文播格 - 马夸尔特法。

## 非线性优化

### 高斯牛顿法

我们将$f(\mathbf{x})$ 进行一阶泰勒展开（注意不是$||f(\mathbf{x})||^2_2$），得到
$$
f(\mathbf{x}+\Delta \mathbf{x})\approx f(\mathbf{x})+\mathbf{J}(\mathbf{x})\Delta \mathbf{x}
$$
回顾一下，我们的目标是寻找$\Delta \mathbf{x}$ 使得$||f(\mathbf{x}+\Delta \mathbf{x})||^2_2$ 最小。这里我们将函数替换成它的一阶泰勒展开，未知量也变成了$\Delta \mathbf{x}$。则目标函数就变成了关于$\Delta \mathbf{x}$ 的一个线性方程：
$$
\Delta \mathbf{x}^*=\arg\min \frac{1}{2}||f(\mathbf{x})+\mathbf{J}\Delta \mathbf{x}||^2
$$
再说一遍，此时$f(\mathbf{x})$ 是由给定的泰勒展开初始值计算出的已知值。类似于上一小节讲的法方程方法，将式(31) 展开并对$\Delta \mathbf{x}$ 求导，令导数为0 即可得：
$$
\mathbf{J}(\mathbf{x})^T\mathbf{J}(\mathbf{x})\Delta \mathbf{x}=-\mathbf{J}(\mathbf{x})^Tf(\mathbf{x})
$$
这是一个关于$\Delta \mathbf{x}$ 的线性方程组，称为**增量方程**。把左边的系数定义为**H**，右边定义为**g**，上式就写成：
$$
\mathbf{H}\Delta \mathbf{x}=\mathbf{g}
$$
可见，高斯牛顿法利用$\mathbf{J}^T\mathbf{J}$ 来近似二阶海塞矩阵从而忽略了海塞矩阵的计算过程。求解增量方程就是整个优化问题的核心所在。基于此，高斯牛顿法的步骤为：

> 1. 给定初始值$\mathbf{x}_0$；
> 2. 对于第k 次迭代，求解雅可比矩阵**J** 和误差f(**x**~k~)；
> 3. 求解增量方程 $\mathbf{H}\Delta \mathbf{x}=\mathbf{g}$；
> 4. 若$\Delta \mathbf{x}_k$ 足够下则停止迭代；
> 5. 否则，令$\mathbf{x}_{k+1}=\mathbf{x}_k + \Delta \mathbf{x}_k$，继续第2 步。

从算法步骤可以看出，求解增量方程是最重要的。原则上，它要求$\mathbf{J}^T\mathbf{J}$ 是可逆（并且是正定）的。但实际中，计算所得的$\mathbf{J}^T\mathbf{J}$ 常常是半正定的。因此，在高斯牛顿法中，可能出现$\mathbf{J}^T\mathbf{J}$ 是奇异阵或者病态的情况。此时求的$\Delta\mathbf{x}$ 不稳定，算法可能不收敛。就算**H** 满足要求，回想泰勒展开的一个隐含前提是：近视只在$\mathbf{x}_k$ 附近有效。如果我们求得的步长$\Delta\mathbf{x}$ 太大，也会使得近似不够准确。

