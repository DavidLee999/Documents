# 决策树

## 基础

决策树是基于树结构来进行决策的。决策过程中的每个判定问题都是对样本某个属性的*测试*。每个测试的结果或是导出最终结论，或是导出进一步的判定问题，其**考虑范围**是*在上次决策结果的限定范围内*。

**性质**：一棵决策树包含一个根结点、若干个内部结点和若干个叶结点；叶结点对应于决策结果，其他每个结点则对应于一个属性测试；每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集。从根结点到每个叶结点的路径对应了一个判定测试序列。

**基本流程**：*分而治之*。

![屏幕快照 2018-03-03 16.04.50](../../../../../Downloads/屏幕快照 2018-03-03 16.04.50.png)

显然，决策树的生成是一个递归过程。**递归的三种情形**：

1. 当前节点包含的样本全属于同一个类别，无需划分；
2. 当前属性集为空，或所有样本在所有属性上取值相同，无法划分；-> **叶节点**，类别为所含样本最多的类别（*后验分布*）；
3. 当前节点包含的样本集合为空，不能划分； -> **叶节点**，类别为其父节点所含样本最多的类别（先验分布）；

## 划分选择

决策树的**关键**是如何选择最优划分属性。随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的*纯度*(purity)越来越高。

### 信息增益

**信息熵** (information entropy) 是度量样本集合纯度最常用的一种指标。假定当前样本集合D 中第k 类样本所占**比例**为p~k~，则D 的信息熵定义为：
$$
Ent(D) = -\sum_{k = 1}^{|Y|}p_k \log_2 p_k
$$
显然，Ent(D)  的最小值为0，最大值为$\log_2|Y|$。Ent(D) 的值越小，D 的纯度越高。

样本的离散属性a 可能有V 个可能取值$\{a^1, a^2, \dots,a^V\}$。若使用a 来对样本集D 进行划分，则会产生**V个分支节点**，第v 个分支包含了D 中左右在属性a 上取值为a^v^ 的样本，记为D^v^。考虑到不同分支节点所包含的样本数不同，给分支节点赋予权重$\frac{|D^v|}{|D|}$。则可以计算出利用属性a 来对D 进行划分所获得的*信息增益* (information gain)。
$$
Gain(D, a) = Ent(D)-\sum_{v = 1}^V\frac{|D^v|}{|D|}Ent(D^v)
$$
一般而言，信息增益越大，则意味着使用属性a 来进行划分所获得的*纯度提升*越大。

**信息增益对可取值数目较多的属性有所偏好**。

### 增益率

$$
Gain\_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}
$$

$$
IV(a) = -\sum_{v = 1}^V\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}
$$

IV 称为属性a 的*固有值* (intrinsic value)。属性a 的可能取值数目越多，则IV 通常会越大。**增益率对可能取值数目较少的属性有所偏好**。可以采用一种**启发式**的方法：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

### 基尼指数

数据集D 的纯度可以用基尼值来衡量：
$$
Gini(D)=\sum_{k=1}^{|Y|}\sum_{k^\prime \ne k}p_k p_{k^\prime}=1-\sum_{k=1}^{|Y|}p^2_k
$$
直观地说，Gini(D) 反映了从数据集D 中随机抽取两个样本，其类别标记不一致的概率。因此，Gini(D) 越小，D 的纯度越高。

属性a 的*基尼指数*定义为：
$$
Gini\_index(D, a) = \sum_{v = 1}^V\frac{|D^v|}{|D|}Gini(D^v)
$$
因此，在候选属性集合A 中，选取使得换分后基尼指数最小的属性作为最优划分属性。