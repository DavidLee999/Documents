# 模型评估

## 经验误差与过拟合

- **错误率** (error rate)：分类错误的样本数a占样本总数m的比例$\frac{a}{m}$;
- **精度** (accuracy)：$1-\frac{a}{m}$;

学习器的实际预测输出与样本的真实输出之间的差异称为误差 (error)；在训练集上的误差称为*“训练误差”*（training error）或*“经验误差”*；在新样本上的误差称为*“泛化误差”*（generalization error）。显然，我们想的到的是**泛化误差小**的学习器，但实际能做的是努力使**经验误差最小化**。

- **过拟合**（overfitting）：我们实际希望的，是在新样本上能表现得很好的学习器。为了达到这个 目的，应该从训练样本中尽可能学出适用于所有潜在样本的"**普遍规律**"，这 样才能在遇到新样本时做出正确的判别。然而，当学习器把训练样本学得"太好"了的时候，很可能巳经把**训练样本自身的一些特点**当作了所有潜在样本都会具有的一般性质，这样就会导致**泛化性能下降**。
- **欠拟合**（underfitting）：对训练样本的一般性质尚未学好。

有**多种因素**可能导致过拟合，其中最常见的就是学习能力过于强大。而学习能力是否“过于强大”，是由学习算法和数据内涵共同决定的。而欠拟合通常是由学习能力低下造成的。**过拟合是无法彻底避免的**，所能做的只是“缓解”，或者说减小期风险。

## 评估方法

通常，使用一个*"测试集"*（testing set）来测试学习器对新样本的判别能力，然后以测试集上的*"测试误差"*（testing error）作为泛化误差的近似。测试集应该尽可能与训练集**互斥**， 即测试样本尽量不在训练集中出现、未在训练过程中使用过。

假设我们只有一个包含m个样例的数据集$D={(x_1, y_1), (x_2, y_2), …, (x_m, y_m)}$，对其进行适当的处理以产生训练即S 和测试集T。

### 留出法

*"留出法"*（hold-out）直接将数据集D 划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，即 $D=S\cup T$， $S\cap T=\emptyset$。在 S 上训练出模型后，用T 来评估其测试误差，作为对泛化误差的估计。

**NOTE**：

- 训练/测试集的划分要尽可能**保持数据分布的一致性**，避免因数据划分过程中引入额外的偏差而对最终结果产生影响。常用的用于保留类别比例的采样方式通常称为*“分层采样”*。
- 使用留出法时，一般采用若干次随机划分、重复进行实验评估后**取平均值**作为留出法的评估结果。

**问题**

- 我们希望模型是用D训练出来的，但留出法需要划分训练/测试集。从*“偏差-方差”*的角度来看：测试集小时，评估结果的方差较大；训练集小时，评估结果的偏差较大。常见做法式将大约2/3 ～ 4/5的样本用于训练，剩余样本用于测试。

### 交叉验证法

*“交叉验证法”*（cross validation）先将数据集D划分为**k个大小相似的互斥子集**，即$D = D_1 \cup D_2 \cup D_3 \cup … \cup D_k, D_i \cap D_j = \emptyset (i \neq j)$. 然后每次用k - 1 个子集的并集作为训练集，余下的那个子集作为测试集。从而进行k 次训练和测试，最终发挥k 次测试结果的均值。也称为*k折交叉验证*（k-fold cross validation）。

![Snip20171215_1](/Users/lipenghua/Downloads/Snip20171215_1.png)

与留出法相似，子集的划分有多种方式。**为了减小因样本划分而引入的误差**，k折交叉验证通常随机使用不同的划分重复p次，称为*p次k折交叉验证*（与p * k 次留出法类似）。

假定数据集D包含m个样本，若令k = m，则得到*”留一法“*（LOO）。

### 自助法

我们希望评估的是用D训练出的模型，而且前两种方法都存在训练样本减少的问题。*”自助法“*是一个比较好的解决方案。

**定义**：给定包含m个样本的数据集D，采样方法为：每次随机挑选一个样本，将**其拷贝**放入数据集D^'^ ，再将至放回D中。重复m次后则得到含有m个样本的数据集D^'^。该方法亦称为*”可重复采样“*或*”有放回采样“*。

显然，有一部分样本会在D^'^中重复出现而有一些不出现。样本在m次采样中始终不被采到的概率为$(1 - \frac{1}{m})^m$，取极限后得到

$$\lim\limits_{m\to\infty} (1 - \frac{1}{m}) ^ m \to \frac{1}{e} \approx 0.368$$

即通过自主采样，D中约有36.8%的样本未出现在D^'^中。此时D^'^为训练集，D\D^'^为测试集。

**适用于**数据集较小，难以有效划分训练/测试集。

## 调参与最终模型

在进行模型评估与选择时，除了要对适用学习算法进行选择，还需对算法参数进行设定，这就是通常所说的*"参数调节"*（parameter tuning）。

**常用做法**是对每个参数选定一个**范围**和**变化步长**，例如在 [0, 0.2] 范围内以 0.05 为步长。

## 性能度量

*“性能度量”*（performance measure）：衡量模型泛化能力。

要评估学习器f 的性能，就要把学习器的预测结果f(x) 与真实标记y 进行比较。例如回归任务重常用的性能度量是“均方误差”（mean suqared error）

$$E(f;D)=\frac{1}{m} \sum\limits_{i=1}^{m}(f(x_i)-y_i)^2$$

### 错误率与精度

对于样例集D，分类错误率为

$$E(f;D)=\frac{1}{m}\sum\limits_{i=1}^{m} \Pi(f(x_i) \neq y_i)$$

精度则为

$$acc(f;D)=\frac{1}{m} \sum\limits_{i=1}^{m} \Pi(f(x_i)=y_i)=1-E(f;D)$$

### 查准率、查全率与F1

将样例根据其真实类别与学习器预测类别的组合划分为*真正例* (true positive) 、*假正例* (false positive) 、*真反例* (true negative) 、 *假反例* (false negative) 四种情形。咳得混淆矩阵：

![Snip20171215_2](/Users/lipenghua/Downloads/Snip20171215_2.png)

*查全率* R（recall）和*查准率* P（precision）分别定义为

$$P=\frac{TP}{TP+FP}$$ （真正例占所有预测正例的比例。精确度。）

$$R=\frac{TP}{TP+FN}$$ （真正例占所有正例的比例。能检测出多少正例。）

查准率和查全率是一对**矛盾**的度量，只有在简单任务中二者才会都很高。

在很多情形札我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器认为"最可能 "是正例的样本。按此顺序逐个把样本作为正例进行预测 ，则每次可以计算出当前的查全率、查准率。**以查准率为纵轴、查全率为横轴作图** ，就得到了*查准率-查全率曲线*，简称 *"P-R曲线"*。

![Snip20171215_3](/Users/lipenghua/Downloads/Snip20171215_3.png)

**比较**：

- 若一个学习器的 P-R 曲线被另一个学习器的曲线完全"包住" ， 则可断言后者的性能优于前者；
- 如果两个学习器的 P-R 曲线发生了交叉，这时一个比较合理的判据是比较 P-R 曲线节面积的大小。

**平衡点**（BEP）：查全率=查准率时的取值。

**F1度量**：$F1=\frac{2*P*R}{P+R}=\frac{2*TP}{样例总数+TP-TN}$. F1度量时查全率和查准率的**调和平均**：$\frac{1}{F1}=\frac{1}{2} (\frac{1}{P}+\frac{1}{R})$.

如果对查全率和查准率的重视程度有所不同，则可以使用$F_\beta$度量：$F_\beta =\frac{(1+\beta ^2) * P * R}{(\beta ^2 * P) + R}$，即**加权调和平均**：$\frac{1}{F_\beta}=\frac{1}{1+F_\beta}(\frac{1}{P}+\frac{\beta ^2}{R})$。其中$\beta>0$度量了**查全率对查准率的相对重要性**：$\beta>1$，查全率更重要；$\beta<1$，查准率更重要。

若进行多次训练，则会得到**多个二分类混淆矩阵**，则

1. 先计算各个混淆矩阵的查准率和查全率，再计算平均值。就得到*“宏查准率*”（macro-P）、*“宏查全率”*（macro-R）以及相应的*”宏F1“*（macro-F1）：

   $$macro \text{-}P=\frac{1}{n} \sum\limits_{i=1}^{n}P_i$$

   $$macro \text{-}R=\frac{1}{n} \sum\limits_{i=1}^{n}R_i$$

   $$macro\text{-}F1=\frac{2*macro-P*macro_R}{macro-P+macro-R}$$

2. 先将各混淆矩阵的对应元素进行平局，得到TP, FN, TN, FN 的平均值，再计算出*“微查准率*”（micro-P）、*“微查全率”*（micro-R）以及相应的*”微F1“*（micro-F1）

   $$micro\text{-}P=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}$$

   $$macro\text{-}R=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}$$

   $$macro\text{-}F1=\frac{2*micro-P*micro_R}{micro-P+micro-R}$$

### ROC和AUC

很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与 一个*分类阔值*(threshold)进行比较，若大于阈值值则分为正类，否则为反类。

实际上，根据这个实值或概率预测结果，我们可将测试样本进行**排序**，"最可能"是正例的排在最前面， "最不可能"是正例的排在最后面。这样，分类过程就相当于在这个排序中以某个*"截断点"* (cut point)将样本分为两部分，前一部分判作正例，后一部分则判作反例。若更重视"查准率"，则可选择排序中靠前的位置进行截断；若更重视"查全率"，则可选择靠后的位置进行截断。

**定义**：*ROC* 全称是"受试者工作特征" (Receiver Operating Characteristic) 曲线。与 P-R 曲线相似，根据学习器的预测结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次计算出两个重要量的值，分别以它们为横、纵坐标作图'就得到了 "ROC 曲线“。ROC 曲线的纵轴是*"真正例率"* ($TPR=\frac{TP}{TP+FN}$，**同查全率**) ，横轴是 *“假正例率”* ($FPR=\frac{FP}{TN+FP}$)。

![Snip20171215_5](/Users/lipenghua/Downloads/Snip20171215_5.png)

显然，对角线对应于*”随机猜测“*；点（0, 1）对应于**把所有正例排在所有反例之前**的*”理想模型“*。

**绘图过程**：:给定 m+ 个正例和m- 个反例，根据学习器预测结果对样例进行排序，然后把分类阔值设为最大，即**把所有样例均预测为反例**，此时真正例率和假正例率均为 0， 在坐标 (0, 0) 处标记一个点然后，**将分类阐值依次设为每个样例的预测值**，即依次将每个样例划分为正例.设前一个标记点坐标为 (x, y) ，当前若为真正例，则对应标记点的坐标为 ($x, y+\frac{1}{m\text{+}}$) ;当前若为假正例，则对应标记点的坐标为 ($x+\frac{1}{m \text{-}}, y$) ，然后用线段连接相邻点即得。

**比较**：

- 若一个学习器的 ROC 曲线被另一个学习器的曲线完全"包住"， 则可断言后者的性能优于前者；

- 若两个学习器的 ROC 曲线发生交叉，较为合理的判据是比较 ROC 曲线下的面积，即 *AUC* (Area Under ROC Curve)。

  $$AUC=\frac{1}{2} \sum\limits_{i=1}^{m-1}(x_{i+1}-x_i) \cdot (y_i+y_{i+1})$$

形式化地看，AUC 考虑的是样本预测的**排序质量**，因此它与**排序误差**紧密相连。

给定 m+ 个正例和 m- 个反例，令 D+ 和 D- 分别表示正、反例集合，则*排序"损失"* (loss)定义为

$$l_{rank}=\frac{1}{m^+ \cdot m^-} \sum\limits_{x^+ \in D^+} \sum\limits_{x^- \in D^-}(\Pi(f(x^+)<f(x^-) + \frac{1}{2}\Pi(f(x^+)=f(x^-))))$$

即考虑每一对正、反例，若正例的预测值小于反例，则记一个罚分；若相等，则记 0.5 个罚分。容易看出，$l_{rank}$对应的是 ROC 曲线之上的面积：$AUC=1-l_{rank}$.

### 代价敏感错误率与代价曲线

为权衡不同类型错误所造成的不同损失，可为错误赋予*"非均等代价"*(unequal cost)。

则对于一个*“代价矩阵”*（cost matrix），cost~ij~ 表示将第 i 类样本预测为第 j 类样本的代价.一般来说 ，cost~ii~ = 0.

![Snip20171215_6](/Users/lipenghua/Downloads/Snip20171215_6.png)

在非均等代价下，我们所希望的不再是简单地最小化错误次数，而是希望**最小化"总体代价"** (total cost)。*”代价敏感“*（cost-sensitive）错误率为：

$$E(f;D;cost)=\frac{1}{m}(\sum\limits_{x_i \in D^+} \Pi (f(x_i) \neq y_i) \cdot cost_{01} + \sum\limits_{x_i \in D^-} \Pi (f(x_i) \neq y_i) \cdot cost_{10})$$

在非均等代价下，ROC 曲线不能直接反映出学习器的期望总体代价，而*"代价曲线"* (cost curve) 则可达到该目的。

## 比较检验

有了实验评估方法和性能度量，就能对学习器的性能进行评估比较了：先使用某种实验评估方法测得学习器的某个性能度量结果，然后对这些结果进行比较。

但比较时存在几个**问题**：

- 我们希望比较泛化性能，得到的却是测试集上的性能；
- 测试集上的性能和测试集本身的选择有很大关系；
- 机器学习算法本身具有一定的随机性。

**统计假设检验**：基于假设检验结果可推断出，若在测试集上观察到学习器 A 比 B 好，则 A 的泛化性能是否在统计意义上优于 B，以及这个结论的把握有多大。

### 假设检验

假设检验中的"假设"是对学习器*泛化错误率分布*的某种**判断或猜想**。现实任务中我们并不知道学习器的泛化错误率，只能获知其测试错误率$\hat\epsilon$。泛化错误率与测试错误率未必相同，但二者接近的概率应该很大。

泛化错误率为$\epsilon$ 的学习器在一个样本上犯错的概率是$\epsilon$；测试错误率$\hat\epsilon$意味着在 m 个测试样本中恰有$\hat\epsilon \cdot m$ 个被误分类；泛化错误率为$\epsilon$ 的学习器将其中m^'^ 个样本误分类 、其余样本全部分类正确的概率是 $\epsilon ^{m^{'}} \cdot (1-\epsilon)^{(m-m^{'})}$ 。

由此可估算出其恰**将$\epsilon \cdot m$ 个样本误分类**的概率：

$$P(\hat\epsilon;\epsilon)=\binom{m}{\hat\epsilon \cdot m} \epsilon^{\hat\epsilon \cdot m}(1-\epsilon)^{m-\hat\epsilon \cdot m}$$

这也表达了在包含m 个样本的测试集上，泛化错误率为$\epsilon$ 的学习器被测得测试错误率为$\hat\epsilon$ 的概率。

给定测试错误率，则解$\frac{\partial{P(\hat\epsilon;\epsilon)}}{\partial \epsilon}=0$ 可知，$P(\hat\epsilon;\epsilon)$在$\epsilon=\hat\epsilon$时最大，$|\epsilon - \hat\epsilon|$增大$P(\hat\epsilon;\epsilon)$ 减小。

**二项检验**：考虑$\epsilon \le \epsilon_0$，则在$1-\alpha$ 的概率内所呢能观测到的最大错误率为：

$$\bar\epsilon=max$$ $$\epsilon$$  s.t.  $$\sum\limits_{i=\epsilon_0 \cdot m + 1}^{m} \binom{m}{i} \epsilon ^i \cdot (1-\epsilon)^{m-i} < \alpha$$

这里$1-\alpha$ 反应了结论的*置信度*。

此时若测试错误率$\epsilon$ 小于临界值$\bar\epsilon$，则根据二项检验可得出结论:在$\alpha$ 的显著度下，假设$\epsilon \le \epsilon_0$ 不能被拒绝，即能以$1-\alpha$ 的置信度认为，学习器的泛化错误率不大于$\epsilon_0$； 否则该假设可被拒绝，即在$\alpha$ 的显著度下可认为学习器的泛化错误率大于$\epsilon_0$。

假设利用交叉验证法进行**多次训练/测试**，得到多个错误率，可以使用***t检验***。 k个测试错误率$\hat\epsilon_1, \hat\epsilon_2, …, \hat\epsilon_k$，平均测试错误率$\mu$和方差$\sigma^2$为：

$$\mu=\frac{1}{k}\sum\limits_{i-1}^{k}\hat\epsilon_i$$

$$\sigma^2=\frac{1}{k-1}\sum\limits_{i=1}^{k}(\hat\epsilon_i-\mu)^2$$

考虑到这 k 个测试错误率可看作泛化错误率$\epsilon_0$的独立采样，则变量

$$\tau_t=\frac{\sqrt{k}(\mu-\epsilon_0)}{\sigma}$$

服从自由度为k - 1的t 分布，如下图所示。

![Snip20171215_7](/Users/lipenghua/Downloads/Snip20171215_7.png)

对于假设“$\mu=\epsilon_0$”和显著度$\alpha$，可计算出当测试错误率均值为$\epsilon_0$时，在$1-\alpha$	概率内能观测到的**最大错误率**，即临界值。若平均错误率$\mu$与$\epsilon_0$之差位于临界值范围$[t_{-\alpha /2}, t_{\alpha /2}]$内，则不能拒绝假设"$\mu=\epsilon_0$"，即可认为**泛化错误率为$\epsilon_0$，置信度为$1-\alpha$.

### 交叉验证t检验

对于两个学习器A和B，若使用k折交叉验证法得到的测试错误率分别为$\epsilon^A_1, \epsilon^A_2, …, \epsilon^A_k$和$,\epsilon^B_1, \epsilon^B_2, …, \epsilon^B_k$，则可用交叉验证*“成对t检验”*（paired t-test）来进行比较检验。

**基本思想**：若两个学习器的性能相同，则他们使用相同的训练/测试集得到的**测试错误率应该相同**，即$\epsilon^A_i=\epsilon^B_i$.

**具体流程**：对k 折交叉验证产生的k 对测试错误率：先对每对结果求差，$\Delta_i=\epsilon^A_i-\epsilon^B_i$；若两个学习器性能相同，则**差值均值应为零**。根据差值$\Delta_1, \Delta_2, …, \Delta_k$来对"学习器 A 与 B 性能相同"这个假设做 t 检验，**计算出差值的均值$\mu$和方差$\sigma^2$**，在显著度$\alpha$下，若变量$\tau_t=|\frac{\sqrt{k} \cdot \mu}{sigma}|$小于临界值$t_{\frac{\alpha}{2},k-1}$，则假设不能被拒绝，即**两个学习器的性能没有显著差别**；否则平均错误率较小的那个学习器性能较优。

要进行有效的假设检验，**一个重要的前提**是测试错误率均为泛化错误率的**独立采样**。但不同轮次的训练集会有重叠，所以测试错误率**并不独立**，会导致**过高估计假设成立的概率**。

**解决：5*2交叉验证**：5 次 2 折交叉验证，在每次 2 折交叉验证之前随机将数据打乱，使得 5 次交叉验证中的数据划分不重复。对两个学习器 A 和 B，第 i 次 2 折交叉验证将产生**两对测试错误率**，我们对它们分别求差，得到第 1 折上的差值$\Delta_i^1$和第 2 折上的差值$\Delta_i^2$。为缓解测试错误率的非独立性，我们**仅计算第 1 次 2 折交叉验证的两个结果的平均值$\mu=\frac{1}{2} \cdot (\Delta_1^1+\Delta_1^2)$**，但对每次 2 折实验的结果都计算出其方差$\sigma_i^2=(\Delta_i^1 - \frac{\Delta_i^1 + \Delta_i^2}{2})^2+(\Delta_i^2 - \frac{\Delta_i^1 + \Delta_i^2}{2})^2$。变量$\tau_t=\frac{\mu}{\sqrt{0.2 \cdot \sum\limits_{i=1}^5}\sigma_i^2}$服从自由度为5的t分布。

### McNemar检验

对二分类问题，使用留出法不仅可估计出学习器 A 和 B 的测试错误率，还可获得两学习器分类结果的差别，即两者都正确、都错误、一个正确另一个错 误的样本数。

![Snip20171215_8](/Users/lipenghua/Downloads/Snip20171215_8.png)

若二者性能相同，则e~01~=e~10~。变量|e~01~ - e~10~|应当服从均值为1、方差为e~01~ + e~10~的正态分布。变量$\tau_{\chi^2}=\frac{(|e_{01}-e{10}|-1)^2}{e_{01}+e{10}|}$服从自由度为1的$\chi^2$分布（卡方分布，标准正态分布变量的平方）。给定显著度$\alpha$，当以上变量值小于临界值$\chi^2_\alpha$，不能拒绝假设，即两学习期的性能没有显著差别；否者平均错误率较小的那个学习器性能较优。

### Friedman检验与Nemenyi后续检验

当利用一组数据对**多个算法**进行比较时，

- 一是在每个数据集上分别进行两两比较；
- 二是使用基于算法排序对Friedman检验。

（待补充）

## 偏差与方差

**解释**：对学习算法除了通过实验估计其泛化性能，人们往往还希望了解它**"为什么"**具有这样的性能。*"偏差方差分解"* (bias-variance decomposition) 是解释学习算法泛化性能的一种重要工具。

算法在不同训练集上学得的结果很可能不同，即便这些训练集是来自同一个分布。

设有训练样本x，数据集上的对应标记为y~D~，y为x 的真实标记，f(x;D) 为模型对预测输出。以回归问题为例（statistics inference）：学习算法的期望预测为：
$$\bar f(x)=E_D[f(x;D)]$$

使用相同样本数的不同训练集产生的方差为：
$$var(x)=E_D[(f(x;D)-\bar f(x))^2]$$

噪声为：

$$\varepsilon^2=E_D[(y_D-y)^2]$$

偏差（期望输出于真实标记的差别）为：

$$bias^2(x)=(\bar f(x)-y)^2$$

假定噪声期望为零，即$E_D[(y_D-y)]=0$，算法的泛化期望误差：

![Snip20171215_9](/Users/lipenghua/Downloads/Snip20171215_9.png)

于是，

$$E(f;D)=bias^2(x)+var(x)+\varepsilon^2$$

**各项含义**：**偏差**度量了学习算法的期望预测与真实结果的偏离程度，即刻画了**学习算法本身的拟合能力**;**方差**度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了**数据扰动所造成的影响**；**噪声**则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了**学习问题本身的难度**。

**偏差-方差分解说明**：泛化性能是由**1）**学习算法的能力、**2）**数据的充分性以及**3）**学习任务本身的难度所共同决定的。给定学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。

但**二者是有冲突的**，称为*偏差-方差窘境*（bias - variance dilemma）。

![Snip20171215_10](/Users/lipenghua/Downloads/Snip20171215_10.png)

给定学习任务，假定我们能控制学习算法的训练程度：**训练不足时**，学习器的拟合能力不够强，训练数据的扰动不足以便学习器产生显著变化，此时**偏差主导了泛化错误率**；**训练程度加深**， 学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学到，**方差逐渐主导了泛化错误率**；**训练程度充足**后，学习器的拟合能力已非常强，**训练数据的轻微扰动都会导致学习器发生显著变化**，若训练数据自身的、非全局的特性被学习器学到了，则将发生过拟合。