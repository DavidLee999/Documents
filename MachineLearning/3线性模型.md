# 线性模型

## 基本形式

给定由 *d* 个属性描述的示例 x = (x~1~; x~2~; … ;x~d~), 其中 x~i~ 是 x 在第 i 个属性上的取值，线性模型(linear model) 试图学得一个通过属性的**线性组合**来进行预测的函数：
$$
f(\mathbf{x}) = \omega_1 x_1 + \omega_2 x_2 + ... + \omega_d x_d + b
$$
向量形式：
$$
f(\mathbf{x})=\mathbf{\omega}^T \cdot \mathbf{x}+b
$$
在 $\mathbf{\omega}$ 和 $b$ 学得后，模型就得以确定。

**优点**：

- 线性模型形式简单、易于建模；
- 许多功能更为强大的非线性模型 (nonlinear model) 可在线性模型的基础上，通过引入**层级结构**或**高维映射**而得；
- 由于 $\mathbf{\omega}$ 直观表达了各属性在预测中的重要性，因此线性模型有很好的可解释性(comprehensibility)。

## 线性回归

### 闭式解的方式

**定义**：给定数据集 $D=\{ (\mathbf{x_1}, y_1), (\mathbf{x_2}, y_2), …, (\mathbf{x_3}, y_3)\}$，"线性回归" (linear regression)试图学得一个线性模型，通过对输入的特征进行加权求和和一个bias term 来尽可能准确地预测实值输出标记.

**NOTE**：对于离散属性，若属性值间存在"序" (order)关系，可通过连续化将其转化为连续值；若属性值间不存在序关系，假定有 k 个属性值，则通常转化为 k 维二值向量。

**目标**：$f(x_i)=\omega_ix_i+b$ 使得 $f(x_i) \approx y_i$

而确定 $\omega$ 和 $b$ 的**关键**在于衡量 f(x) 和 y 之间的差别。*均方误差*是回归任务中最常用的性能度量。均方误差具有很好的几何意义，对应于*欧式距离*。基于均方误差来进行模型求解的方法称为*最小二乘法*。

设$E_{w,b}=\sum_{i=1}^{m}(y_i-\omega x_i -b)$. 将之分别对 $\omega$ 和 $b$ 求导并令等式为零即可得到结果。

> 这里$E_{w,b}$ 是关于 $\omega$ 和 $b$ 的凸函数。对区间 [a, b] 上定义的函数 f，若它对区间中任意两点 x~1~, x~2~ 均有 $f(\frac{x_1+x_2}{2}) \le \frac{f(x_1)+f(x_2)}{2}$，则称 f 为区间 [a, b] 上的**凸函数**。对于实数集上的函数，可通过**求二阶导数**来判别：若二阶导数在区间上非负，则称为凸函数；若二阶导数在区间上恒大于零则称为严格凸函数。

对于多元线性回归，设 $\hat{\mathbf{\omega}}=(\mathbf{\omega}; b)$, $\mathbf{y}=(y_1;y_2; … ; y_m)$ 而数据集表示为

$$\mathbf{X}=\begin{bmatrix} \mathbf{x}_1^T & 1 \\ \mathbf{x}_2^T & 1 \\ \vdots & \vdots \\ \mathbf{x}_m^T & 1 \end{bmatrix}$$

则有
$$
\hat \omega^*=\mathop{\arg \min}_{\hat \omega} (\mathbf{y}-\mathbf{X}\mathbf{\hat \omega})^T (\mathbf{y}-\mathbf{X}\mathbf{\hat \omega})
$$
令$E_\hat\omega=(\mathbf{y}-\mathbf{X}\mathbf{\hat \omega})^T (\mathbf{y}-\mathbf{X}\mathbf{\hat \omega})$ 并对 $\hat\omega$ 求导可得到：
$$
\frac{\partial E_\hat\omega}{\partial \hat\omega}=2\mathbf{X}^T (\mathbf{X} \hat\omega-\mathbf{y})
$$
令上式为零即可得到 $\hat\omega$ 最优解的闭式解。当$\mathbf{X}^T \mathbf{X}$为满秩矩阵，则可得
$$
\hat\omega^*=(\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^T \mathbf{y}
$$
则线性回归的模型为
$$
f(\hat x_i)=\hat x_i^T(\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^T \mathbf{y}
$$
对于非满秩的情况，最常见的接发则是引入正则化（regularization）项。

上式的计算复杂度为 O(n^2.4^) 到 O(n^3^). 它的好处是对训练集内的数据量大小（m）是线性呢。只要内存能够放下，闭式解可以**高效地处理大数据集**。但对于特征多的数据（n维向量）,n*n 大小矩阵的求逆则太过费时。

### 梯度下降法

梯度下降法的**思想**就是不断**迭代调整参数使得代价函数最终达到最小**。

![屏幕快照 2018-01-15 11.34.10](../../../Downloads/屏幕快照 2018-01-15 11.34.10.png)

其中一个重要的参数就是**学习率**，它决定了迭代时的步长。如果学习率太小，则可能收敛速度太慢；若学习率太大，则可能造成震荡。

![屏幕快照 2018-01-15 11.40.41](../../../Downloads/屏幕快照 2018-01-15 11.40.41.png)

![屏幕快照 2018-01-15 11.40.53](../../../Downloads/屏幕快照 2018-01-15 11.40.53.png)

对于梯度下降法，有两个主要的**问题**：

- 收敛到局部最小值；
- 可能在某一处平坦的“高原”上不断地迭代而无法到达全局最小值。

![屏幕快照 2018-01-15 11.43.52](../../../Downloads/屏幕快照 2018-01-15 11.43.52.png)

但是，对于线性回归来说，其代价函数（均方误差(MSE)）是一个凸函数，这就意味着它**只有一个全局最小值而没有局部最小值**。而且，它是一个连续函数，即它的斜率不会突然变化。这就保证了梯度下降法可以任意逼近全局最小值。

在使用梯度下降法还需要注意的一点是**all features should have a similar scale**。

![屏幕快照 2018-01-15 11.54.20](../../../Downloads/屏幕快照 2018-01-15 11.54.20.png)

上图的左图中，feature 1 和 feature 2 具有相同的规模(scale)；而在右图中，feature 1 的规模比feature 2 小（因为在右图中，it takes a large change in $\theta_1$ to affect the cost function, which is why the bowl is elongated along the $\theta_1$ axis.

最后，梯度下降法也反映出训练一个模型就是**searching for a combination of model parameters 来最小化代价函数**。这个查找是在**模型的参数空间**中进行的：一个模型的参数越多，它的参数空间的维度就越大，而查找显然就更费时。

#### 批量梯度下降

要使用梯度下降法，需要计算**代价函数相对于每个参数$\theta_i$ 的导数**。

均方误差的**定义**为：
$$
MSE(\mathbf{X}, h_\theta)=\frac{1}{m}\sum\limits_{i=1}^m(\theta ^T\cdot x^{(i)}-y^{(i)})^2
$$
对$\theta_j$ 求导数可得
$$
\frac{\partial}{\partial \theta_j}MSE(\theta)=\frac{2}{m}\sum\limits_{i=2}^m(\theta^T\cdot x^{(i)}-y^{(i)})x_j^{(i)}
$$
若我们一次性对所有的参数进行求导，则可得到*梯度向量*(gradient vector) $\nabla_\theta MSE(\theta)$:
$$
\nabla_\theta=\begin{pmatrix}
\frac{\partial}{\partial \theta_0}MSE(\theta) \\
\frac{\partial}{\partial \theta_1}MSE(\theta) \\
\vdots \\
\frac{\partial}{\partial \theta_n}MSE(\theta)
\end{pmatrix}
=\frac{2}{m}\mathbf{X}^T\cdot (\mathbf{X}\cdot\theta-\mathbf{y})
$$
注意到利用这个式子做梯度下降在计算时使用了**整个数据集**。所以，当数据集很大时，批量梯度下降会很慢。

利用梯度向量，每次迭代时只需向**梯度方向的反方向**优化即可（$\eta$ 为学习率）：
$$
\theta^{(next\ step)}=\theta - \eta\nabla_\theta MSE(\theta)
$$
为了找到合适的学习率，我们可以使用网格搜索（grid search）。另一个比较好平衡计算的方法是**设定较大的迭代次数，但一旦梯度向量小于某个常量则跳出循环**。

#### 随机梯度下降

*随机梯度下降*在每次迭代过程中只选取训练集中的某一个实例利用式(8) 来计算梯度并完成更新。

但随机梯度下降的**问题**是：它每次迭代代价函数都有可能走高或走低，**decreasing only on average**. 且当迭代停止是，有可能并不会停在最低点上而是在其附近。

![屏幕快照 2018-01-16 20.34.30](../../../Downloads/屏幕快照 2018-01-16 20.34.30.png)

但其**优点**是：当代价函数不规则时，随机梯度下降会帮它跳出局部最小值。

解决的**办法**就是：逐步减小学习率。这就是**模拟退火**算法：一开始时使用较大的迭代步长（来加快进度和跳出局部最小值），然后不断减小学习率以减小迭代步长，保证算法可以 settle at the global minimum.

而用来计算学习率的函数就称为*learning schedule*.

随机梯度下降的**另一个问题**就是其选取数据的随机性：这导致有些数据会被选取好几次而**有些数据从来用不到**。**解决办法**就是每次选取前都**对数据集进行“洗牌”**。

#### Mini-batch Gradient Descent

Mini-batch gradient descent是前两者的结合：每次选取一个**小的随机数据集**来计算梯度向量。**好处**之一就是能够充分利用GPU等硬件提供的资源来优化矩阵计算。

### 多项式回归

当简单的线性模型无法满足要求时，可以考虑**对数据集进行处理**。将数据中的每个特征的**高次项**作为**新的特征**加入到数据中。再利用这些数据来训练一个模型。

除了能够**拟合复杂的模型**之外，多项式回归还可以找到**特征之间的联系**。因为 polynomialFeatures also adds all combinations of features up to the given degree. 利用找到的参数就可以审视数据中特征间的联系。

但**缺点**就是特征空间**维度爆炸**：*PolynomialFeatures(degree = n)* transforms an array containing *n* features into an array containing $\frac{(n+d)!}{d!n!}$ features.

#### 学习曲线（learning curve）



线性模型虽简单，却有丰富的变化。譬如说，假设我们认为示例所对应的输出标记是在指数尺度上变化，那就可将输出标记的对数作为线性模型逼近的目标， 即
$$
ln\ y=\mathbf{\omega}^T \mathbf{x}+b
$$
这就是*对数线性回归*（log-linear regression）。

更一般地，考虑单调可微函数 g，令 $y=g^{-1}(\mathbf{\omega}^T \mathbf{x} + b)$. 即可得到*广义线性模型*（generalized linear model）。而 g 称为*联系函数*（link function）。

## 对数几率回归

前一节讨论了利用线性模型进行回归学习。但若要进行分类任务，则可利用广义线性模型：利用一个**单调可微**的函数将真实标记y 和线性回归模型的预测值联系起来。

考虑二分类任务，显然，输出标记$y \in \{0, 1\}$. 而线性模型的预测值z 是一个实值。所以我们需要将实值z 转换为0/1 值。最理想的当然是**单位阶跃函数**。
$$
y =
\begin{cases}
0, & z < 0 \\
0.5, & z = 0 \\
1, & z > 0
\end{cases}
$$
但显然，阶跃函数时不连续的，所以不能作为$g^-(\cdot)$。所以我们需要找到一个单调可微的*替代函数*，**对数几率函数**（logistic function）正是这样一个函数。从下图可以看出，对数几率函数是一个Sigmoid函数（即S 型函数）。
$$
y =\frac{1}{1+e^{-z}}=\frac{1}{1+e^{-(\mathbf{\omega^T\mathbf{x}+b})}}
$$
![屏幕快照 2018-01-08 14.22.17](../../../Downloads/屏幕快照 2018-01-08 14.22.17.png)

将上式取对数可得：
$$
\ln \frac{y}{1-y}=\mathbf{\omega}^T\mathbf{x}+b
$$
将y 视为样本作为正例的可能性，则1 - y 就是其反例的可能性。二者的比值就是几率。对它取对数就是对数几率。

所以式9 是用线性回归模型的预测结果取逼近真实标记的对数几率，因此其对应的模型称为**对数几率回归**或**逻辑回归**。这种方法有很多**优点**：

- 它是对**分类可能性**进行建模，无需事先假设数据分布，避免了分布不准确带来的问题；
- 它不是仅预测出“类别”，而是得到**近似概率预测**；
- 对率函数是**任意阶可导**的**凸函数**，良好的数学性质使其可以用于许多数值优化算法。

### 求解

将式9 中的y 视为类后验概率估计$p(y=1\ |\ \mathbf{x})$, 就可以将式10 重写为
$$
\ln \frac{p(y=1|\ \mathbf{x})}{p(y=0|\ \mathbf{x})}=\mathbf{\omega}^T\mathbf{x}+b
$$
就有
$$
\begin{align}
p(y=1\ |\ \mathbf{x})=\frac{e^{\mathbf{\omega}^T\mathbf{x}+b}}{1+e^{\mathbf{\omega}^T\mathbf{x}+b}} \\
p(y=0\ |\ \mathbf{x})=\frac{1}{1+e^{\mathbf{\omega}^T\mathbf{x}+b}}
\end{align}
$$
我们可以通过**极大似然法**（maximum likelihood）来对参数进行估计。给定数据集$\{(\mathbf{x}_i,\ y_i)\}^m_{i=1}$, 对率回归模型最大化*对数似然*：
$$
l(\mathbf{\omega},b)=\sum\limits_{i=1}^m\ln p(y_i|\ \mathbf{x};\ \mathbf{\omega},b)
$$
即**令每个样本属于其真实标记的概率越大越好**。记$\mathbf{\beta}=(\mathbf{\omega};b),\hat{\mathbf{x}}=(\mathbf{x};\ 1)$, z 就可以写成$\mathbf{\beta}^T \hat{\mathbf{x}}$. 式10 中的似然项可以写成
$$
p(y_i|\ \mathbf{x};\ \mathbf{\omega},b)=y_ip_1(\hat{\mathbf{x}}_i;\mathbf{\beta})+(1-y_i)p_0(\hat{\mathbf{x}}_i;\mathbf{\beta})
$$
将式15 代入式14，并根据式12 和13 可知，最大化式14 等价于最小化
$$
l(\mathbf{\beta})=\sum\limits_{i=1}^m(-y_i\mathbf{\beta}^T\hat{\mathbf{x}}_i+\ln(1+e^{\mathbf{\beta}^T \hat{\mathbf{x}}_i})
$$
式16 是关于$\mathbf{\beta}$ 的高阶**可导连续凸函数**，可以根据*凸优化理论*求解。