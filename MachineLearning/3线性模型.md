# 线性模型

## 基本形式

给定由 *d* 个属性描述的示例 x = (x~1~; x~2~; … ;x~d~), 其中 x~i~ 是 x 在第 i 个属性上的取值，线性模型(linear model) 试图学得一个通过属性的**线性组合**来进行预测的函数：
$$
f(\mathbf{x}) = \omega_1 x_1 + \omega_2 x_2 + ... + \omega_d x_d + b
$$
向量形式：
$$
f(\mathbf{x})=\mathbf{\omega}^T \cdot \mathbf{x}+b
$$
在 $\mathbf{\omega}$ 和 $b$ 学得后，模型就得以确定。

**优点**：

- 线性模型形式简单、易于建模；
- 许多功能更为强大的非线性模型 (nonlinear model) 可在线性模型的基础上，通过引入**层级结构**或**高维映射**而得；
- 由于 $\mathbf{\omega}$ 直观表达了各属性在预测中的重要性，因此线性模型有很好的可解释性(comprehensibility)。

## 线性回归

**定义**：给定数据集 $D=\{ (\mathbf{x_1}, y_1), (\mathbf{x_2}, y_2), …, (\mathbf{x_3}, y_3)\}$，"线性回归" (linear regression)试图学得一个线性模型以尽可能准确地预测实值输出标记.

**NOTE**：对于离散属性，若属性值间存在"序" (order)关系，可通过连续化将其转化为连续值；若属性值间不存在序关 系，假定有 k 个属性值，则通常转化为 k 维二值向量。

**目标**：$f(x_i)=\omega_ix_i+b$ 使得 $f(x_i) \approx y_i$

而确定 $\omega$ 和 $b$ 的**关键**在于衡量 f(x) 和 y 之间的差别。*均方误差*是回归任务中最常用的性能度量。均方误差具有很好的几何意义，对应于*欧式距离*。基于均方误差来进行模型求解的方法称为*最小二乘法*。

设$E_{w,b}=\sum_{i=1}^{m}(y_i-\omega x_i -b)$. 将之分别对 $\omega$ 和 $b$ 求导并令等式为零即可得到结果。

> 这里$E_{w,b}$ 是关于 $\omega$ 和 $b$ 的凸函数。对区间 [a, b] 上定义的函数 f，若它对区间中任意两点 x~1~, x~2~ 均有 $f(\frac{x_1+x_2}{2}) \le \frac{f(x_1)+f(x_2)}{2}$，则称 f 为区间 [a, b] 上的**凸函数**。对于实数集上的函数，可通过**求二阶导数**来判别：若二阶导数在区间上非负，则称为凸函数；若二阶导数在区间上恒大于零则称为严格凸函数。

对于多元线性回归，设 $\hat{\mathbf{\omega}}=(\mathbf{\omega}; b)$, $\mathbf{y}=(y_1;y_2; … ; y_m)$ 而数据集表示为

$$\mathbf{X}=\begin{bmatrix} \mathbf{x}_1^T & 1 \\ \mathbf{x}_2^T & 1 \\ \vdots & \vdots \\ \mathbf{x}_m^T & 1 \end{bmatrix}$$

则有
$$
\hat \omega^*=\mathop{\arg \min}_{\hat \omega} (\mathbf{y}-\mathbf{X}\mathbf{\hat \omega})^T (\mathbf{y}-\mathbf{X}\mathbf{\hat \omega})
$$
令$E_\hat\omega=(\mathbf{y}-\mathbf{X}\mathbf{\hat \omega})^T (\mathbf{y}-\mathbf{X}\mathbf{\hat \omega})$ 并对 $\hat\omega$ 求导可得到：
$$
\frac{\partial E_\hat\omega}{\partial \hat\omega}=2\mathbf{X}^T (\mathbf{X} \hat\omega-\mathbf{y})
$$
令上式为零即可得到 $\hat\omega$ 最优解的闭式解。当$\mathbf{X}^T \mathbf{X}$为满秩矩阵，则可得
$$
\hat\omega^*=(\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^T \mathbf{y}
$$
则线性回归的模型为
$$
f(\hat x_i)=\hat x_i^T(\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^T \mathbf{y}
$$
对于非满秩的情况，最常见的接发则是引入正则化（regularization）项。

线性模型虽简单，却有丰富的变化。譬如说，假设我们认为示例所对应的输出标记是在指数尺度上变化，那就可将输出标记的对数作为线性模型逼近的目标， 即
$$
ln\ y=\mathbf{\omega}^T \mathbf{x}+b
$$
这就是*对数线性回归*（log-linear regression）。

更一般地，考虑单调可微函数 g，令 $y=g^{-1}(\mathbf{\omega}^T \mathbf{x} + b)$. 即可得到*广义线性模型*（generalized linear model）。而 g 称为*联系函数*（link function）。

## 对数几率回归

前一节讨论了利用线性模型进行回归学习。但若要进行分类任务，则可利用广义线性模型：利用一个**单调可微**的函数将真实标记y 和线性回归模型的预测值联系起来。

考虑二分类任务，显然，输出标记$y \in \{0, 1\}$. 而线性模型的预测值z 是一个实值。所以我们需要将实值z 转换为0/1 值。最理想的当然是**单位阶跃函数**。
$$
y =
\begin{cases}
0, & z < 0 \\
0.5, & z = 0 \\
1, & z > 0
\end{cases}
$$
但显然，阶跃函数时不连续的，所以不能作为$g^-(\cdot)$。所以我们需要找到一个单调可微的*替代函数*，**对数几率函数**（logistic function）正是这样一个函数。从下图可以看出，对数几率函数是一个Sigmoid函数（即S 型函数）。
$$
y =\frac{1}{1+e^{-z}}=\frac{1}{1+e^{-(\mathbf{\omega^T\mathbf{x}+b})}}
$$
![屏幕快照 2018-01-08 14.22.17](../../../Downloads/屏幕快照 2018-01-08 14.22.17.png)

将上式取对数可得：
$$
\ln \frac{y}{1-y}=\mathbf{\omega}^T\mathbf{x}+b
$$
将y 视为样本作为正例的可能性，则1 - y 就是其反例的可能性。二者的比值就是几率。对它取对数就是对数几率。

所以式9 是用线性回归模型的预测结果取逼近真实标记的对数几率，因此其对应的模型称为**对数几率回归**或**逻辑回归**。这种方法有很多**优点**：

- 它是对**分类可能性**进行建模，无需事先假设数据分布，避免了分布不准确带来的问题；
- 它不是仅预测出“类别”，而是得到**近似概率预测**；
- 对率函数是**任意阶可导**的**凸函数**，良好的数学性质使其可以用于许多数值优化算法。

### 求解

将式9 中的y 视为类后验概率估计$p(y=1\ |\ \mathbf{x})$, 就可以将式10 重写为
$$
\ln \frac{p(y=1|\ \mathbf{x})}{p(y=0|\ \mathbf{x})}=\mathbf{\omega}^T\mathbf{x}+b
$$
就有
$$
\begin{align}
p(y=1\ |\ \mathbf{x})=\frac{e^{\mathbf{\omega}^T\mathbf{x}+b}}{1+e^{\mathbf{\omega}^T\mathbf{x}+b}} \\
p(y=0\ |\ \mathbf{x})=\frac{1}{1+e^{\mathbf{\omega}^T\mathbf{x}+b}}
\end{align}
$$
我们可以通过**极大似然法**（maximum likelihood）来对参数进行估计。给定数据集$\{(\mathbf{x}_i,\ y_i)\}^m_{i=1}$, 对率回归模型最大化*对数似然*：
$$
l(\mathbf{\omega},b)=\sum\limits_{i=1}^m\ln p(y_i|\ \mathbf{x};\ \mathbf{\omega},b)
$$
即**令每个样本属于其真实标记的概率越大越好**。记$\mathbf{\beta}=(\mathbf{\omega};b),\hat{\mathbf{x}}=(\mathbf{x};\ 1)$, z 就可以写成$\mathbf{\beta}^T \hat{\mathbf{x}}$. 式10 中的似然项可以写成
$$
p(y_i|\ \mathbf{x};\ \mathbf{\omega},b)=y_ip_1(\hat{\mathbf{x}}_i;\mathbf{\beta})+(1-y_i)p_0(\hat{\mathbf{x}}_i;\mathbf{\beta})
$$
将式15 代入式14，并根据式12 和13 可知，最大化式14 等价于最小化
$$
l(\mathbf{\beta})=\sum\limits_{i=1}^m(-y_i\mathbf{\beta}^T\hat{\mathbf{x}}_i+\ln(1+e^{\mathbf{\beta}^T \hat{\mathbf{x}}_i})
$$
式16 是关于$\mathbf{\beta}$ 的高阶**可导连续凸函数**，可以根据*凸优化理论*求解。