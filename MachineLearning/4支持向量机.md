# 支持向量机

## 间隔与支持向量

给定训练样本集$D=\{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \ldots, (\mathbf{x}_m,y_m\},\ y_i\in\{-1, +1\}$，分类的**基本想法**就是基于训练集D 在样本空间中找到一个划分超平面，将不同类别的样本分开。

划分超平面可用如下方程描述：
$$
\mathbf{\omega}^T\mathbf{x}+b=0
$$
其中$\omega$ 为法向量，b 是位移量。样本空间中任意点x 到超平面的距离可以写为：
$$
r=\frac{|\omega^T\mathbf{x}+b|}{||\omega||}
$$
若超平面能将训练样本正确分类，有：
$$
\begin{cases}
\omega^T\mathbf{x}+b \ge+1,\ y_i=+1 \\
\omega^T\mathbf{x}+b \le+1,\ y_i=-1 \\
\end{cases}
$$
![屏幕快照 2018-01-24 15.22.46](/Users/lipenghua/Downloads/屏幕快照 2018-01-24 15.22.46.png)

可以看到距离超平面最近的几个训练样本可以使上式的等式成立。这些样本被称为**支持向量** (support vector)。两个*异类*支持向量到超平面的距离之和为：
$$
\gamma=\frac{2}{||\omega||}
$$
被称为**间隔** (margin)。

想找到具有**最大间隔** (maximum margin) 的划分超平面，亦即找到满足式(3) 中约束的参数$\omega$ 和b 使得$\gamma$ 最大：
$$
\min_{\omega, b}\frac{1}{2}||\omega||^2\ \ s.t.\ y_i(\omega^T\mathbf{x}_i+b)\ge1
$$
这就是*支持向量机* (Support Vector Machine) 的基本型。

## 对偶问题

求解式(5) 来得到最大间隔划分超平面的模型。对式(5) 使用*拉格朗日乘子法*可得到其*对偶问题*。将式(5) 的每条约束添加拉格朗日乘子$\alpha_i\ge0$ 有：
$$
L(\omega, b, \mathbf{\alpha})=\frac{1}{2}||\omega||^2+\sum\limits_{i=1}^m\alpha_i(1-y_i(\omega^T\mathbf{x}_i+b))
$$
令$L(\omega,b,\alpha)$ 对$\omega$ 和b 的偏导为0 可得：
$$
\begin{align}
\omega=\sum\limits_{i=1}^m\alpha_iy_i\mathbf{x}_i \\
0 = \sum\limits_{i=1}^m\alpha_iy_i
\end{align}
$$
将式(7) 代入式(6) 将$\omega$ 和b 消去，在考虑式(8) 的约束，即可得到式(5) 的对偶问题
$$
\max_\alpha \sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\alpha_i\alpha_jy_iy_j\mathbf{x}^T_i\mathbf{x}_j\ \ s.t.\ \sum\limits_{i=1}^m\alpha_iy_i=0,\ \alpha_i\ge0
$$
解出$\alpha$ 后，求出$\omega$ 和b 即可得到模型
$$
f(\mathbf{x})=\omega^T\mathbf{x}+b=\sum_{i=1}^m \alpha_iy_i\mathbf{x}^T_i\mathbf{x}+b
$$
注意到式(6) 中有不等式约束，因此上述过程需满足KKT 条件，即要求：
$$
\begin{cases}
\alpha_i\ge0 \\
y_if(\mathbf{x}_i)-1 \ge0 \\
\alpha_oy_if(\mathbf{x}_i)-1 =0
\end{cases}
$$
则对任意训练样本$(\mathbf{x}_i,y_i)$，总有$\alpha_i=0$ 或$y_if(\mathbf{x}_i)=1$。

- 若$\alpha_i=0​$，则该样本不会在式(10) 的求和中出现，也就不会对模型有任何影响；
- 若$\alpha_i>0$，则必有$y_if(\mathbf{x}_i)=1$，对应的样本在最大间隔边界上，是一个**支持向量**。

可得SVM 一个**重要性质**：训练完的最终模型只和支持向量有关，其他训练样本无需保留。

式(9) 的**经典解法**有SMO。**基本思路**是：先固定$\alpha_i$ 之外的所有参数，然后求$\alpha_i$ 上的极限。由于存在约束$\sum_{i=1}^m\alpha_iy_i=0$，若固定$\alpha_i$ 之外的其他变量，则$\alpha_i$ 可由其他变量导出。于是SMO 每次选择两个变量$\alpha_i$ 和$\alpha_j$ 并固定其他参数，初始化参数后不断执行以下两个步骤直至收敛：

- 选取一对需更新的变量$\alpha_i$ 和$\alpha_j$；
- 固定$\alpha_i$ 和$\alpha_j$ 意外以外的参数，求解式(9) 获得更新后的$\alpha_i$ 和$\alpha_j$。

**注意到**只要选取的$\alpha_i$ 和$\alpha_j$ 中有一个**不满足**KKT 条件，目标函数就会在迭代后**减小**。*KKT条件的违背程度越大，变量更新后可能导致的目标函数值减幅越大*。所以，SMO 先选取违背KKT 条件程度最大的变量，第二个变量则选择一个使目标函数数值减小最快的变量。为此，SMO 使用了**启发式**策略：使选取的两变量所对应的样本之间的间隔最大。（**解释**：两个变量有很大的差别，与对两个相似的变量进行更新相比，对它们进行更新会带给目标函数值更大的变化。）

仅考虑$\alpha_i$ 和$\alpha_j$ 时，式(9) 的约束 (s.t. 后的部分) 可以写为
$$
\alpha_iy_i + \alpha_j y_j=c,\ \alpha_i \ge 0, \alpha_j \ge 0
$$
其中，
$$
c=-\sum_{k\ne i,j}\alpha_ky_k
$$
是使$\sum_{i=1}^m\alpha_iy_i$ 成立的常数。

用 $\alpha_iy_i+\alpha_jy_j=c$ 消去式(9) 中的变量$\alpha_j$，得到一个关于$\alpha_i$ 对单变量二次规划问题，仅有的约束时$\alpha_i\ge0$。

最后需要求解偏移量b。对任意支持向量$(\mathbf{x}_s, y_s)$ 有
$$
y_i(\sum_{i\in S}\alpha_iy_i\mathbf{x}_i^T\mathbf{x}_s+b)=1
$$
其中S 为所有支持向量的下标集。可以选取**任意**支持向量求解上式来获得b。实际中常常使用**所有**支持向量然后**求取**平均值。
$$
b=\frac{1}{|S|}\sum_{s\in S}(y_s-\sum_{i\in S}\alpha_iy_i\mathbf{x}^T_i\mathbf{x}_s)
$$
