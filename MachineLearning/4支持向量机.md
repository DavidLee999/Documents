# 支持向量机

## 间隔与支持向量

给定训练样本集$D=\{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \ldots, (\mathbf{x}_m,y_m\},\ y_i\in\{-1, +1\}$，分类的**基本想法**就是基于训练集D 在样本空间中找到一个划分超平面，将不同类别的样本分开。

划分超平面可用如下方程描述：
$$
\mathbf{\omega}^T\mathbf{x}+b=0
$$
其中$\omega$ 为法向量，b 是位移量。样本空间中任意点x 到超平面的距离可以写为：
$$
r=\frac{|\omega^T\mathbf{x}+b|}{||\omega||}
$$
若超平面能将训练样本正确分类，有：
$$
\begin{cases}
\omega^T\mathbf{x}+b \ge+1,\ y_i=+1 \\
\omega^T\mathbf{x}+b \le+1,\ y_i=-1 \\
\end{cases}
$$
![屏幕快照 2018-01-24 15.22.46](/Users/lipenghua/Downloads/屏幕快照 2018-01-24 15.22.46.png)

可以看到距离超平面最近的几个训练样本可以使上式的等式成立。这些样本被称为**支持向量** (support vector)。两个*异类*支持向量到超平面的距离之和为：
$$
\gamma=\frac{2}{||\omega||}
$$
被称为**间隔** (margin)。

想找到具有**最大间隔** (maximum margin) 的划分超平面，亦即找到满足式(3) 中约束的参数$\omega$ 和b 使得$\gamma$ 最大：
$$
\min_{\omega, b}\frac{1}{2}||\omega||^2\ \ s.t.\ y_i(\omega^T\mathbf{x}_i+b)\ge1
$$
这就是*支持向量机* (Support Vector Machine) 的基本型。

## 对偶问题

求解式(5) 来得到最大间隔划分超平面的模型。对式(5) 使用*拉格朗日乘子法*可得到其*对偶问题*。将式(5) 的每条约束添加拉格朗日乘子$\alpha_i\ge0$ 有：
$$
L(\omega, b, \mathbf{\alpha})=\frac{1}{2}||\omega||^2+\sum\limits_{i=1}^m\alpha_i(1-y_i(\omega^T\mathbf{x}_i+b))
$$
令$L(\omega,b,\alpha)$ 对$\omega$ 和b 的偏导为0 可得：
$$
\begin{align}
\omega=\sum\limits_{i=1}^m\alpha_iy_i\mathbf{x}_i \\
0 = \sum\limits_{i=1}^m\alpha_iy_i
\end{align}
$$
将式(7) 代入式(6) 将$\omega$ 和b 消去，在考虑式(8) 的约束，即可得到式(5) 的对偶问题
$$
\max_\alpha \sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\alpha_i\alpha_jy_iy_j\mathbf{x}^T_i\mathbf{x}_j
$$
$$
s.t.\ \sum\limits_{i=1}^m\alpha_iy_i=0,\ \alpha_i\ge0
$$

解出$\alpha$ 后，求出$\omega$ 和b 即可得到模型
$$
f(\mathbf{x})=\omega^T\mathbf{x}+b=\sum_{i=1}^m \alpha_iy_i\mathbf{x}^T_i\mathbf{x}+b
$$
注意到式(5) 中有不等式约束，因此上述过程需满足KKT 条件，即要求：
$$
\begin{cases}
\alpha_i\ge0 \\
y_if(\mathbf{x}_i)-1 \ge0 \\
\alpha_oy_if(\mathbf{x}_i)-1 =0
\end{cases}
$$
则对任意训练样本$(\mathbf{x}_i,y_i)$，总有$\alpha_i=0$ 或$y_if(\mathbf{x}_i)=1$。

- 若$\alpha_i=0​$，则该样本不会在式(10) 的求和中出现，也就不会对模型有任何影响；
- 若$\alpha_i>0$，则必有$y_if(\mathbf{x}_i)=1$，对应的样本在最大间隔边界上，是一个**支持向量**。

可得SVM 一个**重要性质**：训练完的最终模型只和支持向量有关，其他训练样本无需保留。

式(9) 的**经典解法**有SMO。**基本思路**是：先固定$\alpha_i$ 之外的所有参数，然后求$\alpha_i$ 上的极限。由于存在约束$\sum_{i=1}^m\alpha_iy_i=0$，若固定$\alpha_i$ 之外的其他变量，则$\alpha_i$ 可由其他变量导出。于是SMO 每次选择两个变量$\alpha_i$ 和$\alpha_j$ 并固定其他参数，初始化参数后不断执行以下两个步骤直至收敛：

- 选取一对需更新的变量$\alpha_i$ 和$\alpha_j$；
- 固定$\alpha_i$ 和$\alpha_j$ 意外以外的参数，求解式(9) 获得更新后的$\alpha_i$ 和$\alpha_j$。

**注意到**只要选取的$\alpha_i$ 和$\alpha_j$ 中有一个**不满足**KKT 条件，目标函数就会在迭代后**减小**。*KKT条件的违背程度越大，变量更新后可能导致的目标函数值减幅越大*。所以，SMO 先选取违背KKT 条件程度最大的变量，第二个变量则选择一个使目标函数数值减小最快的变量。为此，SMO 使用了**启发式**策略：使选取的两变量所对应的样本之间的间隔最大。（**解释**：两个变量有很大的差别，与对两个相似的变量进行更新相比，对它们进行更新会带给目标函数值更大的变化。）

仅考虑$\alpha_i$ 和$\alpha_j$ 时，式(9) 的约束 (s.t. 后的部分) 可以写为
$$
\alpha_iy_i + \alpha_j y_j=c,\ \alpha_i \ge 0, \alpha_j \ge 0
$$
其中，
$$
c=-\sum_{k\ne i,j}\alpha_ky_k
$$
是使$\sum_{i=1}^m\alpha_iy_i$ 成立的常数。

用 $\alpha_iy_i+\alpha_jy_j=c$ 消去式(9) 中的变量$\alpha_j$，得到一个关于$\alpha_i$ 对单变量二次规划问题，仅有的约束时$\alpha_i\ge0$。

最后需要求解偏移量b。对任意支持向量$(\mathbf{x}_s, y_s)$ 有
$$
y_i(\sum_{i\in S}\alpha_iy_i\mathbf{x}_i^T\mathbf{x}_s+b)=1
$$
其中S 为所有支持向量的下标集。可以选取**任意**支持向量求解上式来获得b。实际中常常使用**所有**支持向量然后**求取**平均值。
$$
b=\frac{1}{|S|}\sum_{s\in S}(y_s-\sum_{i\in S}\alpha_iy_i\mathbf{x}^T_i\mathbf{x}_s)
$$

**注意**：SVM 对于样本的尺度十分敏感。如下图所示，左图中竖轴的尺度远远大于横轴的尺度，所以SVM 求得的最大间隔会尽量接近水平（以获得最大的宽度）。

![sensitivity_to_feature_scales_plot](../../../Downloads/sensitivity_to_feature_scales_plot.png)

## 软间隔与正则化

**问题**：

- 硬间隔SVM要求数据必须是线性可分的；
- 对outliers 十分敏感。有一个outlier。

但是

- 现实任务很难确定合适的核函数使训练样本在特征空间中线性可分；
- 即使恰好找到了某个核函数使训练集在特征空间中线性可分，也很难断定这个貌似线性可分的结果不是由于过拟合所造成的 。

**解决**：允许SVM 在一些样本上出错。为此，引入**软间隔** (soft margin) 的概念。

前面我们要求所有样本都要满足式(3)，即所有样本必须划分正确，称为**硬间隔**。*软间隔*则允许某些样本不满足这个约束。

**目标**：在最大化间隔的同事，不满足约束的样本尽可能少：
$$
\min_{\omega, b}\frac{1}{2}||\omega||^2+C\sum\limits_{i=1}^ml_{0/1}(y_i(\omega^T\mathbf{x}_i+b)-1)
$$
其中$C>0$ 是一个常数，$l_{0/1}$ 是*0/1 损失函数*：
$$
l_{0/1}(z)=\begin{cases}
1, z<0\\
0,\ else
\end{cases}
$$
显然，当C 越大，式(17) 要求越多样本都必须满足约束（间隔可能越小），此时接近硬间隔；当C 越小，则允许越多样本不满足（间隔可能越大）。且可以通过减小C 来缓解SVM 的过拟合。

但$l_{0/1}$ 非凸、分连续，数学性质不好，常被其他函数替代，称为*替代损失* (surrogate loss)，这些函数一般是**凸的连续函数**且是$l_{0/1}$ 的**上界**。常用的有：

- hinge 损失：$l_{hinge}(z)=max(0,1-z)$
- 指数损失：$l_{\exp}(z)=\exp(-z)$
- 对率损失：$l_{\log}(z)=\log(1 + \exp(-z))$

![屏幕快照 2018-01-26 14.45.20](../../../Downloads/屏幕快照 2018-01-26 14.45.20.png)

以hinge 损失为例，式(16) 变为：
$$
\min_{\omega, b}\frac{1}{2}||\omega||^2+C\sum\limits_{i=1}^m\max(0, 1-y_i(\omega\mathbf{x}_i+b))
$$
**解释**：

- 若样本分类正确且在间隔外，乘积大于1，相减后为负数，损失为0；
- 若样本分类正确但在间隔内，乘积小于1， 相减后损失在0和1之间；
- 若样本分类错误，乘积为大于1 的负数，损失大于1.

引入**松弛变量** (slack variable) $\xi_i\ge0$，上式可以重写为：
$$
\min_{\omega, b, \xi_i}\frac{1}{2}||\omega||^2 + C\sum_{i=1}^m\xi_i \\
s.t.\ y_i(\omega^T\mathbf{x}_i+b)\ge1-\xi_i,\ \xi_i\ge0
$$
即为**软间隔支持向量机**。

显然，上式中每个样本都有一个对应的松弛变量用以**表征该样本不满足约束(3) 的程度 **。通过拉格朗日乘子法可得：
$$
L(\omega,b,\alpha,\xi,\mu)=\frac{1}{2}||\omega||^2+C\sum_{i=1}^m\xi_i\\
+\sum_{i=1}^m\alpha_i(1-\xi_i-y_i(\omega^T\mathbf{x}_i+b))-\sum_{i=1}^m\mu_i\xi_i
$$
其中，$\alpha_i\ge0,\ \mu_i\ge0$  是拉格朗日乘子。

令上式对$\omega, b, \xi_i$ 对偏导为0 可得：
$$
\begin{cases}
\omega=\sum_{i=1}^m\alpha_iy_i\mathbf{x}_i \\
0 = \sum_{i=1}^m\alpha_iy_i\\
C=\alpha_i+\mu_i
\end{cases}
$$
回代后即可得到其（式(21)）的对偶问题：
$$
\max_{\alpha}\sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_j y_i y_j\mathbf{x}_i^T \mathbf{x}_j
$$

$$
s.t.\ \sum_{i=1}^m \alpha_i y_i=0,\ 0\le \alpha_i \le C
$$



把上式与硬间隔下的对偶问题式(9) 对比可发现二者唯一的**差别**是**对对偶变量的约束不同**：前者比后者多要求$\alpha_i\le C$。

最后，对软间隔支持向量机，KKT 条件要求：
$$
\begin{cases}
\alpha_i\ge0,\ \mu_i\ge0\\
y_if(\mathbf{x}_i)-1+\xi_i\ge0\\
\alpha_i(y_i f(\mathbf{x_i})-1+\xi_i)=0\\
\xi_i \ge0,\ \mu_i\xi_i=0
\end{cases}
$$
所以，对任意训练样本，总有$\alpha_i=0$ 或$y_i f(\mathbf{x}_i)=1-\xi_i$。

- 若$\alpha_i=0$，则该样本对模型没有任何影响；
- 若$\alpha_i>0$，则必有$y_i f(\mathbf{x}_i)=1-\xi_i$。该样本为支持向量。
- 联合式(22) 和式(25)，若$\alpha_i < C$， 则$\mu_i\ge0$；则必有$\xi_i=0$。该样本在最大间隔边界上；
- 若$\alpha_i=C$，则$\mu_i=0$。此时
  - 若$\xi_i\le1$，该样本落在最大间隔内部；
  - 若$\xi_i >1$，该样本被错分。

**说明**软间隔支持向量机的最终模型仅与支持向量有关，即通过采用hinge 损失函数仍保持了稀疏性。

**若采用对率损失函数**，则几乎得到了对率回归模型。二者**异同**：

- 支持向量机与对率回归的优化目标相近，性能也相当；
- 对率回归的优势主要在于其输出具有自然的概率意义；
- 对率回归能直接用于多分类任务，支持向量机为此则需进行推广；
- 对率损失是光滑的单调递减函数，不能导出类似支持向量的概念。因此对率回归的解依赖于更多的训练样本，预测开销更大。

最后，可用其他替代损失函数。所得模型的性质与所用的替代函数相关。但所有模型的**共性**：优化目标中的第一项用来描述划分超平面的"间隔"大小，另一项$\sum_{i=1}^ml(f(\mathbf{x}_i),y_i)$用来表述训练集上的误差。

一般形式为：
$$
\min_{f} \Omega(f)+C\sum_{i=1}^ml(f(\mathbf{x}_i),y_i)
$$
**解释**：$\Omega(f)$ 称为**结构风险** (strutural risk)，用以**描述模型f 的某些性质**，表达了我们希望获得具有何种性质的模型；第二项$\sum_{i=1}^ml(f(\mathbf{x}_i),y_i)$称为**经验风险** (empirical risk)，用以**描述模型与训练数据的契合程度**。而C 用于对二者进行折中。

截止目前，我们都在使用线性SVM。因此也可以用随机梯度下降法来训练一个**线性模型**以逼近SVM，只需将代价函数改为hinge 函数。对软间隔SVM，再引入正则化项并将正则化系数设为$\frac{1}{m\cdot C}$ 即可。

## 核函数

面对不是线性可分的样本集，可将样本从原始空间映射到一个更**高维**的特征空间 ，使得样本在这个特征空间内线性可分。

**NOTE**：如果原始空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本可分。

令$\phi(\mathbf{x})$ 表示将x 映射后的特征向量。则在高维空间中的对应模型为：
$$
f(\mathbf{x})=\omega^T\phi(\mathbf{x})+b
$$
类似于线性SVM，有
$$
\min_{\omega, b}\frac{1}{2}||\omega||^2 \ \ s.t.\ y_i(omega^T\phi(\mathbf{x}_i)+b)\ge1
$$
其对偶问题为
$$
\max_{\alpha}\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m \alpha_i \alpha_j y_i y_j \phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j)
$$

$$
s.t.\ \sum_{i=1}^m\alpha_i y_i=0,\ \alpha_i \ge 0
$$

**问题**：求解式(29) 需要计算$\phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j)$。高维空间的**维数可能很高**，直接计算是很困难的。

**解决**：引入函数
$$
\kappa(\mathbf{x}_i, \mathbf{x}_j)=\langle\phi(\mathbf{x}_i), \phi(\mathbf{x}_j)\rangle=\phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j)
$$
即二者在高维特征空间的内积为它们在**原始样本空间**中通过函数$\kappa(\cdot,\cdot)$ 计算的结果。式(29) 可写为
$$
\max_{\alpha}\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m \alpha_i \alpha_j y_i y_j \kappa(\mathbf{x}_i, \mathbf{x}_j)
$$
求解后可得
$$
\begin{align}
f(\mathbf{x}) & =\omega^T\phi(\mathbf{x})+b \\
 & =\sum_{i=1}^m \alpha_i y_i\phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j) + b \\
 & =\sum_{i=1}^m \alpha_i y_i \kappa(\mathbf{x}_i, \mathbf{x}_j)+b
\end{align}
$$
这里$\kappa(\cdot,\cdot)$ 就是*核函数* (kernel function)。显然，如知道$\phi(\cdot)$ 的具体形式，就可以写出$\kappa(\cdot,\cdot)$.

**定义**：令X 为输入空间，$\kappa(\cdot,\cdot)$ 是定义在X * X 上的**对称函数**，则其为核函数当且仅当对于任意数据D，**核矩阵** K 总是半正定的。
$$
\mathbf{K}=\begin{bmatrix}
\kappa(\mathbf{x}_1, \mathbf{x}_1)  & \ldots & \kappa(\mathbf{x}_1, \mathbf{x}_m) \\
\vdots & & \vdots \\
\kappa(\mathbf{x}_m, \mathbf{x}_1) & ldots & \kappa(\mathbf{x}_m, \mathbf{x}_m)
\end{bmatrix}
$$
该定义说明，只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用。而对于一个半正定核矩阵，总能找到一个与之对应的映射$\phi$。换言之，任何一个核函数都隐式定义了一个称为*再生核希尔伯特空间* (RKHS) 的特征空间。

显然，**特征空间的好坏**对支持向量机的性能至关重要。在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的，而核函数也仅是隐式地走义了这个特征空间。于是，**核函数的选择**成为支持向量机的最大变数。常用的核函数有：

![屏幕快照 2018-01-27 15.31.49](../../../Downloads/屏幕快照 2018-01-27 15.31.49.png)

此外，还可以通过**函数组合**得到新的核函数：

- 若$\kappa_1$ 和$\kappa_2$ 为核函数，其线性组合$\gamma_1 \kappa_1+\gamma_2\kappa_2$ 也是核函数；
- 若$\kappa_1$ 和$\kappa_2$ 为核函数，其直积$\kappa_1\bigotimes\kappa_2(\mathbf{z},\mathbf{x})=\kappa_1(\mathbf{z},\mathbf{x})\kappa_1(\mathbf{z},\mathbf{x})$ 也是核函数；
- 若$\kappa_1$ 为核函数，对于任意函数g(x)，$\kappa(\mathbf{x},\mathbf{z})=g(\mathbf{x})\kappa_1(\mathbf{x},\mathbf{z})g(\mathbf{z})$ 也是核函数。

