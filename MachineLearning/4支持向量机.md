# 支持向量机

## 间隔与支持向量

给定训练样本集$D=\{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \ldots, (\mathbf{x}_m,y_m\},\ y_i\in\{-1, +1\}$，分类的**基本想法**就是基于训练集D 在样本空间中找到一个划分超平面，将不同类别的样本分开。

划分超平面可用如下方程描述：
$$
\mathbf{\omega}^T\mathbf{x}+b=0
$$
其中$\omega$ 为法向量，b 是位移量。样本空间中任意点x 到超平面的距离可以写为：
$$
r=\frac{|\omega^T\mathbf{x}+b|}{||\omega||}
$$
若超平面能将训练样本正确分类，有：
$$
\begin{cases}
\omega^T\mathbf{x}+b \ge+1,\ y_i=+1 \\
\omega^T\mathbf{x}+b \le-1,\ y_i=-1 \\
\end{cases}
$$
![屏幕快照 2018-01-24 15.22.46](4/屏幕快照 2018-01-24 15.22.46.png)

可以看到距离超平面最近的几个训练样本可以使上式的等式成立。这些样本被称为**支持向量** (support vector)。两个*异类*支持向量到超平面的距离之和为：
$$
\gamma=\frac{2}{||\omega||}
$$
被称为**间隔** (margin)。

想找到具有**最大间隔** (maximum margin) 的划分超平面，亦即找到满足式(3) 中约束的参数$\omega$ 和b 使得$\gamma$ 最大：
$$
\min_{\omega, b}\frac{1}{2}||\omega||^2\ \ s.t.\ y_i(\omega^T\mathbf{x}_i+b)\ge1
$$
这就是*支持向量机* (Support Vector Machine) 的基本型。

## 对偶问题

求解式(5) 来得到最大间隔划分超平面的模型。对式(5) 使用*拉格朗日乘子法*可得到其*对偶问题*。将式(5) 的每条约束添加拉格朗日乘子$\alpha_i\ge0$ 有：
$$
L(\omega, b, \mathbf{\alpha})=\frac{1}{2}||\omega||^2+\sum\limits_{i=1}^m\alpha_i(1-y_i(\omega^T\mathbf{x}_i+b))
$$
令$L(\omega,b,\alpha)$ 对$\omega$ 和b 的偏导为0 可得：
$$
\begin{align}
\omega & =\sum\limits_{i=1}^m\alpha_iy_i\mathbf{x}_i \\
0 & = \sum\limits_{i=1}^m\alpha_iy_i
\end{align}
$$
将式(7) 代入式(6) 将$\omega$ 和b 消去，在考虑式(8) 的约束，即可得到式(5) 的对偶问题
$$
\max_\alpha \sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\mathbf{x}^T_i\mathbf{x}_j
$$
$$
s.t.\ \sum\limits_{i=1}^m\alpha_iy_i=0,\ \alpha_i\ge0
$$

解出$\alpha$ 后，求出$\omega$ 和b 即可得到模型
$$
f(\mathbf{x})=\omega^T\mathbf{x}+b=\sum_{i=1}^m \alpha_iy_i\mathbf{x}^T_i\mathbf{x}+b
$$
注意到式(5) 中有不等式约束，因此上述过程需满足KKT 条件，即要求：
$$
\begin{cases}
\alpha_i\ge0 \\
y_if(\mathbf{x}_i)-1 \ge0 \\
\alpha_i(y_if(\mathbf{x}_i)-1) =0
\end{cases}
$$
则对任意训练样本$(\mathbf{x}_i,y_i)$，总有$\alpha_i=0$ 或$y_if(\mathbf{x}_i)=1$。

- 若$\alpha_i=0$，则该样本不会在式(11) 的求和中出现，也就不会对模型有任何影响；
- 若$\alpha_i>0$，则必有$y_if(\mathbf{x}_i)=1$，对应的样本在最大间隔边界上，是一个**支持向量**。

可得SVM 一个**重要性质**：训练完的最终模型只和**支持向量**有关，其他训练样本无需保留。

式(9) 的**经典解法**有SMO（序列最小化）。**基本思路**是：先固定$\alpha_i$ 之外的所有参数，然后求$\alpha_i$ 上的极限。由于存在约束$\sum_{i=1}^m\alpha_iy_i=0$，若固定$\alpha_i$ 之外的其他变量，则$\alpha_i$ 可由其他变量导出。于是SMO 每次选择两个变量$\alpha_i$ 和$\alpha_j$ 并固定其他参数，初始化参数后不断执行以下两个步骤直至收敛：

- 选取一对需更新的变量$\alpha_i$ 和$\alpha_j$；
- 固定$\alpha_i$ 和$\alpha_j$ 以外的参数，求解式(9) 获得更新后的$\alpha_i$ 和$\alpha_j$。

**注意到**只要选取的$\alpha_i$ 和$\alpha_j$ 中有一个**不满足**KKT 条件，目标函数就会在迭代后**减小**。*KKT条件的违背程度越大，变量更新后可能导致的目标函数值减幅越大*。所以，SMO 先选取违背KKT 条件程度最大的变量，第二个变量则选择一个使目标函数数值减小最快的变量。为此，SMO 使用了**启发式**策略：使选取的两变量所对应的样本之间的间隔最大。（**解释**：两个变量有很大的差别，与对两个相似的变量进行更新相比，对它们进行更新会带给目标函数值更大的变化。）

仅考虑$\alpha_i$ 和$\alpha_j$ 时，式(9) 的约束 (s.t. 后的部分) 可以写为
$$
\alpha_iy_i + \alpha_j y_j=c,\ \alpha_i \ge 0, \alpha_j \ge 0
$$
其中，
$$
c=-\sum_{k\ne i,j}\alpha_ky_k
$$
是使$\sum_{i=1}^m\alpha_iy_i=0$ 成立的常数。

用 $\alpha_iy_i+\alpha_jy_j=c$ 消去式(9) 中的变量$\alpha_j$，得到一个关于$\alpha_i$ 对单变量二次规划问题，仅有的约束时$\alpha_i\ge0$。

最后需要求解偏移量b。对任意支持向量$(\mathbf{x}_s, y_s)$ 有
$$
y_s(\sum_{i\in S}\alpha_iy_i\mathbf{x}_i^T\mathbf{x}_s+b)=1
$$
其中S 为所有支持向量的下标集。可以选取**任意**支持向量求解上式来获得b。实际中常常使用**所有**支持向量然后**求取**平均值。
$$
b=\frac{1}{|S|}\sum_{s\in S}(y_s-\sum_{i\in S}\alpha_iy_i\mathbf{x}^T_i\mathbf{x}_s)
$$

**注意**：SVM 对于样本的尺度十分敏感。如下图所示，左图中竖轴的尺度远远大于横轴的尺度，所以SVM 求得的最大间隔会尽量接近水平（以获得最大的宽度）。

![sensitivity_to_feature_scales_plot](4/sensitivity_to_feature_scales_plot.png)

## 软间隔与正则化

**问题**：

- 硬间隔SVM要求数据必须是线性可分的；
- 对outliers 十分敏感。有一个outlier 就有可能分类失败。

但是

- 现实任务很难确定合适的核函数使训练样本在特征空间中线性可分；
- 即使恰好找到了某个核函数使训练集在特征空间中线性可分，也很难断定这个貌似线性可分的结果不是由于过拟合所造成的 。

**解决**：允许SVM 在一些样本上出错或落在间隔内。为此，引入**软间隔** (soft margin) 的概念。

前面我们要求所有样本都要满足式(3)，即所有样本必须划分正确，称为**硬间隔**。*软间隔*则允许某些样本不满足这个约束。

**目标**：在最大化间隔的同时，不满足约束的样本尽可能少（二者相互矛盾，引入C 作为这两个目标平衡）：
$$
\min_{\omega, b}\frac{1}{2}||\omega||^2+C\sum\limits_{i=1}^ml_{0/1}(y_i(\omega^T\mathbf{x}_i+b)-1)
$$
其中$C>0$ 是一个常数，$l_{0/1}$ 是*0/1 损失函数*：
$$
l_{0/1}(z)=\begin{cases}
1, z<0\\
0,\ else
\end{cases}
$$
显然，当C 越大，式(17) 接近式(5)，要求越多样本都必须满足约束（间隔可能越小），此时接近硬间隔；当C 越小，则允许越多样本不满足（间隔可能越大）。且可以通过减小C 来缓解SVM 的过拟合。

但$l_{0/1}$ 非凸、分连续，数学性质不好，常被其他函数替代，称为*替代损失* (surrogate loss)，这些函数一般是**凸的连续函数**且是$l_{0/1}$ 的**上界**。常用的有：

- hinge 损失：$l_{hinge}(z)=max(0,1-z)$
- 指数损失：$l_{\exp}(z)=\exp(-z)$
- 对率损失：$l_{\log}(z)=\log(1 + \exp(-z))$

![屏幕快照 2018-01-26 14.45.20](4/屏幕快照 2018-01-26 14.45.20.png)

以hinge 损失为例，式(16) 变为：
$$
\min_{\omega, b}\frac{1}{2}||\omega||^2+C\sum\limits_{i=1}^m\max(0, 1-y_i(\omega\mathbf{x}_i+b))
$$
**解释**：

- 若样本分类正确且在间隔外，乘积大于1，相减后为负数，损失为0；
- 若样本分类正确但在间隔内，乘积小于1， 相减后损失在0和1之间；
- 若样本分类错误，乘积为绝对值大于1 的负数，损失大于1.

引入**松弛变量** (slack variable) $\xi_i\ge0$ (代表了第i 个样本**不满足约束(3) 的程度**)，上式可以重写为：
$$
\min_{\omega, b, \xi_i}\frac{1}{2}||\omega||^2 + C\sum_{i=1}^m\xi_i \\
s.t.\ y_i(\omega^T\mathbf{x}_i+b)\ge1-\xi_i,\ \xi_i\ge0
$$
即为**软间隔支持向量机**。

通过拉格朗日乘子法可得：
$$
L(\omega,b,\alpha,\xi,\mu)=\frac{1}{2}||\omega||^2+C\sum_{i=1}^m\xi_i\\
+\sum_{i=1}^m\alpha_i(1-\xi_i-y_i(\omega^T\mathbf{x}_i+b))-\sum_{i=1}^m\mu_i\xi_i
$$
其中，$\alpha_i\ge0,\ \mu_i\ge0$  是拉格朗日乘子。

令上式对$\omega, b, \xi_i$ 对偏导为0 可得：
$$
\begin{cases}
\omega=\sum_{i=1}^m\alpha_iy_i\mathbf{x}_i \\
0 = \sum_{i=1}^m\alpha_iy_i\\
C=\alpha_i+\mu_i
\end{cases}
$$
回代后即可得到其（式(20)）的对偶问题：
$$
\max_{\alpha}\sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_j y_i y_j\mathbf{x}_i^T \mathbf{x}_j
$$

$$
s.t.\ \sum_{i=1}^m \alpha_i y_i=0,\ 0\le \alpha_i \le C
$$



把上式与硬间隔下的对偶问题式(9) 对比可发现二者唯一的**差别**是对**对偶变量的约束不同**：前者比后者多要求$\alpha_i\le C$。

最后，对软间隔支持向量机，KKT 条件要求：
$$
\begin{cases}
\alpha_i\ge0,\ \mu_i\ge0\\
y_if(\mathbf{x}_i)-1+\xi_i\ge0\\
\alpha_i(y_i f(\mathbf{x_i})-1+\xi_i)=0\\
\xi_i \ge0,\ \mu_i\xi_i=0
\end{cases}
$$
所以，对任意训练样本，总有$\alpha_i=0$ 或$y_i f(\mathbf{x}_i)=1-\xi_i$。

- 若$\alpha_i=0$，则该样本对模型没有任何影响；
- 若$\alpha_i>0$，则必有$y_i f(\mathbf{x}_i)=1-\xi_i$。该样本为支持向量。
- 联合式(22) 和式(25)，若$\alpha_i < C$， 则$\mu_i\ge0$；则必有$\xi_i=0$。该样本在最大间隔边界上；
- 若$\alpha_i=C$，则$\mu_i=0$。此时
  - 若$\xi_i\le1$，该样本落在最大间隔内部；
  - 若$\xi_i >1$，该样本被错分。

**说明**软间隔支持向量机的最终模型仅与支持向量有关，即通过采用hinge 损失函数仍保持了**稀疏性**。

**若采用对率损失函数**，则几乎得到了对率回归模型。二者**异同**：

- 支持向量机与对率回归的优化目标相近，性能也相当；
- 对率回归的优势主要在于其输出具有自然的概率意义；
- 对率回归能直接用于多分类任务，支持向量机为此则需进行推广；
- 对率损失是光滑的单调递减函数，不能导出类似支持向量的概念。因此对率回归的解依赖于更多的训练样本，预测开销更大。

最后，可用其他替代损失函数。所得模型的性质与所用的替代函数相关。但所有模型的**共性**：优化目标中的第一项用来描述划分超平面的"间隔"大小，另一项$\sum_{i=1}^ml(f(\mathbf{x}_i),y_i)$用来表述训练集上的误差。

一般形式为：
$$
\min_{f} \Omega(f)+C\sum_{i=1}^ml(f(\mathbf{x}_i),y_i)
$$
**解释**：$\Omega(f)$ 称为**结构风险** (strutural risk)，用以**描述模型f 的某些性质**，表达了我们希望获得具有何种性质的模型；第二项$\sum_{i=1}^ml(f(\mathbf{x}_i),y_i)$称为**经验风险** (empirical risk)，用以**描述模型与训练数据的契合程度**。而C 用于对二者进行折中。

截止目前，我们都在使用线性SVM。因此也可以用随机梯度下降法来训练一个**线性模型**以逼近SVM，只需将代价函数改为hinge 函数。对软间隔SVM，再引入正则化项并将正则化系数设为$\frac{1}{m\cdot C}$ 即可。

## 核函数

面对不是线性可分的样本集，可将样本从原始空间映射到一个更**高维**的特征空间 ，使得样本在这个特征空间内线性可分。

**NOTE**：如果原始空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本可分。

令$\phi(\mathbf{x})$ 表示将x 映射后的特征向量。则在高维空间中的对应模型为：
$$
f(\mathbf{x})=\omega^T\phi(\mathbf{x})+b
$$
类似于线性SVM，有
$$
\min_{\omega, b}\frac{1}{2}||\omega||^2 \ \ s.t.\ y_i(\omega^T\phi(\mathbf{x}_i)+b)\ge1
$$
其对偶问题为
$$
\max_{\alpha}\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m \alpha_i \alpha_j y_i y_j \phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j)
$$

$$
s.t.\ \sum_{i=1}^m\alpha_i y_i=0,\ \alpha_i \ge 0
$$

**问题**：求解式(29) 需要计算$\phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j)$。高维空间的**维数可能很高**，直接计算是很困难的。

**解决**：引入函数
$$
\kappa(\mathbf{x}_i, \mathbf{x}_j)=\langle\phi(\mathbf{x}_i), \phi(\mathbf{x}_j)\rangle=\phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j)
$$
即二者在高维特征空间的内积为它们在**原始样本空间**中通过函数$\kappa(\cdot,\cdot)​$ 计算的结果。式(29) 可写为
$$
\max_{\alpha}\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m \alpha_i \alpha_j y_i y_j \kappa(\mathbf{x}_i, \mathbf{x}_j)
$$
求解后可得
$$
\begin{align}
f(\mathbf{x}) & =\omega^T\phi(\mathbf{x})+b \\
 & =\sum_{i=1}^m \alpha_i y_i\phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j) + b \\
 & =\sum_{i=1}^m \alpha_i y_i \kappa(\mathbf{x}_i, \mathbf{x}_j)+b
\end{align}
$$
这里$\kappa(\cdot,\cdot)$ 就是**核函数** (kernel function)。显然，如果知道$\phi(\cdot)$ 的具体形式，就可以写出$\kappa(\cdot,\cdot)$.

**定义**：令X 为输入空间，$\kappa(\cdot,\cdot)$ 是定义在X * X 上的**对称函数**，则其为核函数当且仅当对于任意数据D，**核矩阵** K 总是半正定的。
$$
\mathbf{K}=\begin{bmatrix}
\kappa(\mathbf{x}_1, \mathbf{x}_1)  & \ldots & \kappa(\mathbf{x}_1, \mathbf{x}_m) \\
\vdots & & \vdots \\
\kappa(\mathbf{x}_m, \mathbf{x}_1) & \ldots & \kappa(\mathbf{x}_m, \mathbf{x}_m)
\end{bmatrix}
$$
该定义说明，只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用。而对于一个半正定核矩阵，总能找到一个与之对应的映射$\phi$。换言之，任何一个核函数都隐式定义了一个称为*再生核希尔伯特空间* (RKHS) 的特征空间。

显然，**特征空间的好坏**对支持向量机的性能至关重要。在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的，而核函数也仅是隐式地定义了这个特征空间。于是，**核函数的选择**成为支持向量机的最大变数。常用的核函数有：

![屏幕快照 2018-01-27 15.31.49](4/屏幕快照 2018-01-27 15.31.49.png)

此外，还可以通过**函数组合**得到新的核函数：

- 若$\kappa_1$ 和$\kappa_2$ 为核函数，其线性组合$\gamma_1 \kappa_1+\gamma_2\kappa_2$ 也是核函数；
- 若$\kappa_1$ 和$\kappa_2$ 为核函数，其直积$\kappa_1\bigotimes\kappa_2(\mathbf{z},\mathbf{x})=\kappa_1(\mathbf{z},\mathbf{x})\kappa_1(\mathbf{z},\mathbf{x})$ 也是核函数；
- 若$\kappa_1$ 为核函数，对于任意函数g(x)，$\kappa(\mathbf{x},\mathbf{z})=g(\mathbf{x})\kappa_1(\mathbf{x},\mathbf{z})g(\mathbf{z})$ 也是核函数。


以多项式核为例，我们可以加入样本的高次项进入数据集中，但这会有维度爆炸等等问题。使用核技巧可以达到同样的效果而不必真的加入这些高次样本特征。

### Adding Similarity Features

另一个解决非线性问题的技巧是加入利用一个**相似函数**计算的特征。该函数计算每个样本和路标 (landmark) 的相似程度。例如，对于下图中的数据，加入两个路标x~1~ = -2 和x~1~ = 1。相似函数为*高斯径向基函数* (RBF), gamma 系数为0.3：
$$
\phi\gamma(\mathbf{x},\mathbf{l})=\exp(-\gamma||\mathbf{x} - \mathbf{l}||^2)
$$
利用该函数即可计算每个样本的新特征。例如样本x~1~ = -1，它与第一个路标距离为1，与第二个样本距离为2，该样本的新特征即为：x~2~ = exp(-0.3 * 1) = 0.74, x~2~ = exp(-0.3 * 4) = 0.30. 可见新的数据集是线性可分的。

![kernel_method_plot](4/kernel_method_plot.png)

**如何选择路标**：最简单的方法即在每个样本的位置都创建一个路标。则一个有m 个样本，每个样本有n 个特征的数据集会变为一个有m 个样本，m个特征的数据集。

### 高斯核

和多项式方法类似，使用相似方程的方法**适用于任何机器学习算法**。但其一样有计算量大的缺点。在SVM 中，可以直接使用高斯径向基函数核。

```python
rbf_kernel_svm_clf = Pipeline((
    ("scaler", StandardScaler()), 
    ("svm_clf", SVC(kernel="rbf", gamma=5, C=0.001))
)) 

rbf_kernel_svm_clf.fit(X, y)
```

![moons_rbf_svc_plot](4/moons_rbf_svc_plot.png)

可以看到，增加$\gamma$ 使得分类边界变窄，使得每个样本的影响区间变小：分类边界变得更不规则，更靠近样本。因此，$\gamma$ 和C 类似，有**正则化**的效果：若过拟合则减小$\gamma$；否则可以增大它。

**核函数的选择**：As a rule of thumb, 应该先尝试**线性SVM**，特别是数据集很大、特征维度很高的时候。如果数据集大小合适，则可以尝试**高斯径向基函数**。然后，如果时间和算力允许，才挑选适合数据集的特殊核函数，如String kernels 特别适合于文本文件分类或DNA序列。

## 支持向量回归

给定训练样本集$D=\{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \ldots, (\mathbf{x}_m,y_m\},\ y_i\in R$，希望学得一个**回归模型**，使得f(x) 和y 尽可能接近。

对样本 (x, y)，传统回归模型通常直接基于模型输出f(x) 与真实输出y 之间的差别来计算损失，当且仅当f(x) 与y 完全相同时，损失才为零。与此不同，支持向量回归 (Support Vector Regression, SVR)，假设我们能容忍f(x) 与y 之间最多有$\epsilon$ 的偏差，即仅当f(x) 与y 之间的差别绝对值大于$\epsilon$时才计算损失。这相当于以f(x) 为中心，构建了一个**宽度为$2\epsilon$ 的间隔带**。若样本**落入间隔带**，则是预测正确的。*（SVR 的目标与SVM 正好相反，它使尽可能多的样本落在间隔内并尽力限制间隔的大小。）*

![屏幕快照 2018-02-13 09.49.13](../../../../../Downloads/屏幕快照 2018-02-13 09.49.13.png)

SVR 问题可以形式化为：
$$
\min_{\omega, b} \frac{1}{2}||\omega||^2+C\sum_{i=1}^m l_{\epsilon}(f(\mathbf{x}_i)-y_i)
$$
其中C 为正则化常数，$l_{\epsilon}$ 为下图所示的$\epsilon$ - 不敏感损失函数：
$$
l_{\epsilon}(z)=\begin{cases}
0,\ \ if\ |z| \le\epsilon, \\
|z|-\epsilon,\ \ otherwise.
\end{cases}
$$
![屏幕快照 2018-02-13 09.55.24](../../../../../Downloads/屏幕快照 2018-02-13 09.55.24.png)

引入松弛变量$\xi_i$ 和$\hat\xi_i$，则式(38) 重写为：
$$
\min_{\omega, b, \xi_i, \hat\xi_i} \frac{1}{2}||\omega||^2+C\sum_{i=1}^m(\xi_i + \hat\xi_i)
$$

$$
\begin{align}
s.t.\ \ & f(\mathbf{x}_i) - y_i \le \epsilon + \xi_i,\\
 & y_i - f(\mathbf{x}_i) \le \epsilon + \hat\xi_i,\\
 & \xi_i \ge 0,\ \hat\xi_i \ge 0
\end{align}
$$

类似式(21)，引入拉格朗日乘子$\mu_i \ge 0,\hat\mu_i\ge 0,\alpha_i \ge 0,\hat\alpha_i\ge 0$，可得到式(40) 的拉格朗日函数：
$$
L(\omega, b, \alpha, \hat\alpha, \xi, \hat\xi, \mu, \hat\mu)\\
=\frac{1}{2}||\omega||^2 + C\sum_{i=1}^m(\xi_i+\hat\xi_i) - \sum_{i=1}^m\mu_i \xi_i - \sum_{i=1}^m\hat\mu_i \hat\xi_i\\
+\sum_{i=1}^m\alpha_i (f(\mathbf{x}_i)-y_i - \epsilon - \xi_i) + \sum_{i=1}^m \hat\alpha_i (y_i - f(\mathbf{x}_i) - \epsilon - \hat\xi_i)
$$
将式 $f(\mathbf{x}_i)=\omega^T\mathbf{x}+b$ 代入，令上式对$\omega, b, \xi_i, \hat\xi_i$ 的偏导为零即可得：
$$
\begin{align}
\omega & = \sum_{i=1}^m (\hat\alpha_i - \alpha_i)\mathbf{x}_i, \\
0 & = \sum_{i=1}^m (\hat\alpha_i-\alpha_i), \\
C & = \alpha_i+\mu_i,\\
C & = \hat\alpha_i+\hat\mu_i
\end{align}
$$
将上面4 个式子代入式(44) 中，即可得到SVR 的对偶问题：
$$
\max_{\alpha, \hat\alpha}\sum_{i=1}^my_i(\hat\alpha_i - \alpha_i) - \epsilon(\hat\alpha_i+\alpha_i)\\
- \frac{1}{2}\sum_{i=1}^m\sum_{i=1}^m(\hat\alpha_i - \alpha_i)(\hat\alpha_j - \alpha_j)\mathbf{x}^T_i\mathbf{x}_j
$$

$$
s.t.\ \ \sum_{i=1}^m(\hat\alpha_i-\alpha_i)=0,\ \ 0 \le \alpha_i, \hat\alpha_i \le C
$$

上述过程同样需要满足KKT 条件：
$$
\begin{cases}
\alpha_i(f(\mathbf{x}_i)-y_i - \epsilon - \xi_i) = 0,\\
\hat\alpha_i(y_i-f(\mathbf{x}_i)-\epsilon-\hat\xi_i) = 0,\\
\alpha_i \hat\alpha_i=0,\ \xi_i \hat\xi_i = 0, \\
(C-\alpha_i)\xi_i = 0,\ (C - \hat\alpha_i)\hat\xi_i = 0.
\end{cases}
$$
可以看出，当且仅当$f(\mathbf{x}_i)-y_i - \epsilon - \xi_i = 0$ 时，$\alpha_i$ 才可以取非零值；当且仅当$y_i-f(\mathbf{x}_i)-\epsilon-\hat\xi_i = 0$ 时$\hat\alpha_i$ 能取非零值。换言之，仅当样本(x~i~, y~i~) **不落入**$\epsilon$ - 间隔带中，相应的$\alpha_i, \hat\alpha_i$ 才能取非零值。而且，约束$f(\mathbf{x}_i)-y_i - \epsilon - \xi_i=0$ 和$y_i-f(\mathbf{x}_i)-\epsilon-\hat\xi_i=0$ **不能同时成立**，因此$\alpha_i, \hat\alpha_i$ 中至少一个为零。

最后，将式(45) 代入回归式中，得到SVR 的解形如：
$$
f(\mathbf{x})=\sum_{i=1}^m(\hat\alpha_i - \alpha_i)\mathbf{x}^T_i\mathbf{x}+ b
$$
能使上式中的$\hat\alpha_i - \alpha \ne 0$ 的样本即为SVR 的**支持向量**，它们必落在$\epsilon$ - 间隔带**之外** (加入位于间隔带内的样本对模型没有影响)。显然，SVR 的支持向量仅仅是训练样本的一部分，即**解具有稀疏性**。

由KKT 条件(51) 可以看出，对于每个样本都有$(C-\alpha_i)\xi_i = 0$ 和$\alpha_i(f(\mathbf{x}_i)-y_i - \epsilon - \xi_i) = 0$。在得到$\alpha_i$ 后，若$0<\alpha_i<C$，则必有$\xi_i=0$。进而有：
$$
b=y_i+\epsilon-\sum_{i=1}^m(\hat\alpha_i-\alpha_i)\mathbf{x}^T_i\mathbf{x}
$$
因此，在得到$\alpha_i$ 后，理论上可以选择任意满足$0<\alpha_i<C$ 的样本来求解b。实践中常选取多个或所有满足条件的样本来求解再取平均值。

如果考虑特征映射形式$\phi(\mathbf{x}_i)$ 及其对应的核函数$\kappa(\mathbf{x}_i,\mathbf{x}_j)=\phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j)$，SVR 可表示为：
$$
f(\mathbf{x})=\sum_{i=1}^m(\hat\alpha_i-\alpha_i)\kappa(\mathbf{x},\mathbf{x}_i)+b
$$

## Under the hood (补充)

SVM 的决策边界 (decision boundary) 是令决策方程 (decision function) 为零的点 (亦即分割超平面)：对于二维数据来说，分割直线为数据平面和决策方程平面的交线：

![iris_3D_plot](4/iris_3D_plot.png)

**几何解释**：注意到决策方程的斜率为权重向量的模$||\omega||$，且如果我们将斜率除2则相当于将间隔增大2倍 (如下图所示)，所以我们的优化目标为尽力减小$||\omega||$。

![small_w_large_margin_plot](4/small_w_large_margin_plot.png)

SVM 的原问题是一个primal problem，一个primal problem 需满足一定条件才能保证对应对偶问题的解也是原问题的解：the objective function is convex and the inequaliry constraints are continuously differentiable and convex functions.