# 深度学习

##DNN

深度神经网络 (DNN) 意为具有两个或以上隐层的感知机。TensorFlow 有对应的高级API，`tf.contrib.learn.DNNClassifier()`。在此讨论如何利用tf 搭建一个DNN。

### 搭建DNN

#### 建图

1. **参数设置**：输入输出，隐层神经元个数等；

   ```python
   n_inputs = 28*28  # MNIST
   n_hidden1 = 300
   n_hidden2 = 100
   n_outputs = 10
   ```

2. **设置输入输出数据**：使用*占位符* (placeholder)；

   ```python
   X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")
   y = tf.placeholder(tf.int64, shape=(None), name="y")
   ```

   此时，要注意每层输入输出的**维度**：X 为n_instances * n_features 大小的矩阵，**每一行是一个样本**；y 为n_instances * 1 的向量。

3. **创建神经层** (neuron layer)；

   1. 自建

      ```python
      def neuron_layer(X, n_neurons, name, activation=None):
          with tf.name_scope(name):
              n_inputs = int(X.get_shape()[1])
              stddev = 2 / np.sqrt(n_inputs)
              init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)
              W = tf.Variable(init, name="kernel")
              b = tf.Variable(tf.zeros([n_neurons]), name="bias")
              Z = tf.matmul(X, W) + b
              if activation is not None:
                  return activation(Z)
              else:
                  return Z
      ```

      其中，W 为n_features * n_neurons 大小的权值矩阵，**每一列是一个神经元的权值**，初始化技巧后面详述，注意到这里使用截断高斯分布 (truncated_normal) ；b 为bias term；输出为z = X W + b。

   2. 使用tf 函数

      ```python
      tf.layers.dense(...)
      ```

      基本函数和自建函数类似。

4. **搭建隐层和输出层**：注意到输入层只为接受输入，这里不用显式建立；

   ```python
   with tf.name_scope("dnn"):
       hidden1 = neuron_layer(X, n_hidden1, name="hidden1", activation=tf.nn.relu)
       hidden2 = neuron_layer(hidden1, n_hidden2, name="hidden2", activation=tf.nn.relu)
       logits = neuron_layer(hidden2, n_outputs, name="outputs")
   ```

   或者

   ```python
   with tf.name_scope("dnn"):
       hidden1 = tf.layers.dense(X, n_hidden1, name="hidden1", activation=tf.nn.relu)
       hidden2 = tf.layers.dense(hidden1, n_hidden2, name="hidden2", activation=tf.nn.relu)
       logits = tf.layers.dense(hidden2, n_outputs, name="outputs")
   ```

   注意到这里的输入logits 还是不是softmax 的分类结果，这样是为了后面更好地进行损失函数定义、求解。

5. **定义损失函数**：使用*交叉熵* (cross-entropy) 来作为cost function；

   ```python
   with tf.name_scope("loss"):
       xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)
       loss = tf.reduce_mean(xentropy, name="loss")
   ```

6. **训练**：定义优化方法（和设置待优化变量）；

   ```python
   learning_rate = 0.01

   with tf.name_scope("train"):
       optimizer = tf.train.GradientDescentOptimizer(learning_rate)
       training_op = optimizer.minimize(loss)
   ```

7. **评估**：这里使用精度 accuracy 来作为评判标准；

   ```python
   with tf.name_scope("eval"):
       correct = tf.nn.in_top_k(logits, y, 1)
       accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))
   ```

8. **初始化**：最后，初始化数据图（和设置模型保存路径）；

   ```python
   init = tf.global_variables_initializer()
   saver = tf.train.Saver()
   ```

#### 计算

1. **设置参数**：epoch, mini-batch 的大小和数据集准备等；

   ```python
   n_epochs = 400
   n_batches = 50
   ```

2. **建立会话** (session)：

   ```python
   with tf.Session() as sess:
       init.run()
       for epoch in range(n_epochs):
           for iteration in range(mnist.train.num_examples // batch_size):
               X_batch, y_batch = mnist.train.next_batch(batch_size)
               sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
           acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})
           acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})
           print(epoch, "Train accuracy:", acc_train, "Test accuracy:", acc_test)

       save_path = saver.save(sess, "./my_model_final.ckpt")
   ```

3. **复用模型**：加载与使用；

   ```python
   with tf.Session() as sess:
       saver.restore(sess, "./my_model_final.ckpt") # or better, use save_path
       X_new_scaled = mnist.test.images[:20]
       Z = logits.eval(feed_dict={X: X_new_scaled})
       y_pred = np.argmax(Z, axis=1)
   ```



至此，一个简单的DNN 模型就搭建并训练完毕了。

### 参数调节

一个DNN 包含了许多可以调节的参数，包括但不限于：网络的拓扑结构，隐层层数，每层的神经元数量，激活方程和参数初始化方法等等。

对于隐层层数来说，具有多隐层的神经网络具有更高的**parameter efficiency**：相比于浅层神经网络，它们可以**利用 (exponentially) 更少的神经元**来逼近任意复杂的方程，使得**训练速度更快**。此外，深层神经网络具有更好的**泛化能力**。

对于每层的神经元数量，一般遵循**漏斗**原则，即越深（靠近输出）的层拥有越少的神经元。

## 训练DNN

DNN 在训练过程中会面临许多问题，主要可以分为三类：

1. 梯度消失/爆炸；
2. 训练过慢；
3. 过拟合；

其中，这三类问题可以一起通过某种（数学）方法加以解决。

### 梯度爆炸/消失

之前利用梯度下降法来训练神经网络，但这带来了问题：

1. 随着梯度下降法运行到**浅层**神经层，梯度会变得**越来越小**。这就使得浅层神经元的权值**几乎不变**。这就是**梯度消失** (vanishing gradients)；
2. 对于某些损失和激活函数函数，浅层神经元的梯度会**越来越大**（较少发生），使得算法**发散**。这就是**梯度爆炸** (exploding gradients)

导致该问题的**重要原因**之一就是使用Sigmoid 方程作为激活方程和权值随机初始化（使用均值为0标准差为1的高斯分布）。因为这样的策略使得每一层的输出的方差 (variance) 都大于其输入的方差，最后使得Sigmoid 方程饱和 (satured)。而且Sigmoid 方程的均值为0.5而非1.

### 参数初始化

**要求**：数据在**前向**和**反向**过程中都能够恰当的传播，即每层的输入和输出都具有接近的方差且梯度在前向和反向中也有类似的方差。

要完全满足以上要求是比较难的，但依然可以利用较好的参数初始化技巧来完成。

#### Xavier initialization

若使用Sigmoid 方程作为激活方程，常用Xavier 初始化方法：

- 均值为0 的高斯分布 (normal distribution)，标准差为$\sigma = \sqrt{\frac{2}{n_{inputs} + n_{outputs}}}$；
- uniform distribution between -r and r, with $r = \sqrt{\frac{6}{n_{inputs} + n_{outputs}}}$。

其中，n_inputs 和n_outputs 为对应层的输入和输出数目。

#### He initialization

若使用tanh 方程或ReLU 方程或其变体（包括ELU方程）作为激活方程，常使用He 初始化方法：

![屏幕快照 2018-04-13 20.37.29](9/屏幕快照 2018-04-13 20.37.29.png)

### 激活函数

神经网络的激活函数可分为**线形**或**非线性**。非线性激活函数可以使神经网络逼近任意复杂函数。没有激活函数带来的非线性，多层神经网络和单层无异。

#### Sigmoid 方程

$$
\sigma (z) = \frac{1}{1 + e^{-z}}
$$

![sigmoid_saturation_plot](9/sigmoid_saturation_plot.png)

![02907640-6](9/02907640-6.png)

Sigmoid 函数具有三个**缺陷**：

- 易饱和。在输入很大或很小时是，其梯度趋近于0，造成梯度消失；
- 输出不以0（而以0.5）为中心；
- 计算成本较高。

#### tanh 函数

![43148640-7](9/43148640-7.png)

![63485640-8](9/63485640-8.png)

可以看到，tanh (双曲正切) 函数一样会遭遇梯度消失（或称为dying neuron）问题。

#### ReLU 函数

$$
f(x) = \max(0, z)
$$

![06134640-9](9/06134640-9.png)

![14606640-10](9/14606640-10.png)

ReLU (修正线形单元) 函数被认为是**好的默认激活函数**。其计算简单，且在输入为正时不会有梯度消失问题。但它也存在问题：

- 不以0为中心；
- 在前向传导中，若输入的加权和小于0，即x < 0的话，输出恒为0。在后向传播中，其梯度为0，权重无法更新，称为**dying ReLU**。

#### leaky ReLU

$$
f(x) = \max(\alpha x, x)
$$

![leaky_relu_plot](9/leaky_relu_plot.png)



为解决ReLU 方程存在的问题，引入了leaky ReLU 方程，a 通常设置为0.01。它具有ReLu 方程的优点，但也有不连续的缺点。

#### parametric leaky RuLU

不把leaky ReLU 方程中的a 当作参数的话，又可以引出ReLU 的两个变体。

- 一是Randomized leaky ReLU (RReLU)：在训练过程中，在一个取值范围内随意挑选a 的取值，然后在测试/使用时将之固定为均值。该方法可以在一定程度上作为正则化；
- 二是直接讲a 作为学习参数在训练过程中加以学习。

总之，ReLU 是一个很好的默认选择，但也可以试用其他激活函数。

#### ELU 函数

$$
ELU_{\alpha} = 
\begin{cases}
\alpha (\exp (z) - 1), z < 0 \\
z, z \ge 0
\end{cases}
$$

![elu_plot](9/elu_plot.png)

ELU (exponential linear unit) 函数类似于ReLU，但具有以下**优点**：

- 在z < 0 时其取值为负，一来促使输出的均值接近0，而来也减缓梯度消失问题；
- 当z < 0 时其梯度不为0， 避免了dying unit 问题；
- 函数光滑，方便了梯度计算。

但它也具有计算量比ReLU 大的问题。但ELU 可以**加速收敛**，一定程度上缓解了这个问题。

#### SELU 函数

SELU 函数是ELU 函数的改良。在ELU 的基础上增加了一个scaling 并确定了a 的取值。

```python
def selu(z,
         scale=1.0507009873554804934193349852946,
         alpha=1.6732632423543772848170429916717):
    return scale * elu(z, alpha)
```

![selu_plot](9/selu_plot.png)

使用SELU 函数使得即使是有100层的神经网络也能 preserves roughly mean 0 and standard deviation 1 across all layers，并且避免梯度消失/爆炸问题。

### Batch Normalization

BN 是在每一层的数据在**进入激活函数前**对其进行处理，以避免梯度消失/爆炸的问题。本质上，它是为了解决 Internal Covariate Shift problem, which is the problem that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. 它对输入进行zero-centering 和normalizing，并且对结果进行scaling 和shifting。这使得**每一层**多了两个训练参数。

计算流程如下：

1. $\mu_B = \frac{1}{m_B} \sum\limits_{i=1}^{m_B} \mathbf{x}^{(i)}$
2. $\sigma_B^2 = \frac{1}{m} \sum\limits_{i=1}^{m_B} (\mathbf{x}^{(i)} - \mu_B)^2$
3. $\mathbf{\hat x}^{(i)} = \frac{\mathbf{x}^{(i)} - \mu_B}{\sqrt{\mu_B^2} + \epsilon}$
4. $\mathbf{z}^{(i)} = \gamma \mathbf{\hat x}^{(i)} + \beta$

其中，

- $\mu_B$ 是该mini-batch 的经验均值；
- $\sigma_B$ 是标准差；
- $m_B$ 是mini-batch 的大小；

若没有mini-batch 可供计算，可以试用整个训练集的均值和方差。

最后，BN 也可以作为正则化的手段之一。但它确实增加了网络的计算复杂性。

```python
training = tf.placeholder_with_default(False, shape=(), name='training')

hidden1 = tf.layers.dense(X, n_hidden1, name="hidden1") # activation default is None
bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)
bn1_act = tf.nn.elu(bn1)
```

