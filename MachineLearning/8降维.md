# 降维

## k 近邻学习

k 近邻 (k-Nearest Neighbor，简称kNN) 学习的**工作机制**：给定测试样本，基于某种距离度量找出训练集中与其最靠近的k 个训练样本，然后基于着k 个邻居的信息来进行预测。在分类中常使用投票法；在回归任务中可使用平均法。也可以将距离考虑在内，使用加权平均或者加权投票。

kNN 其实是**懒惰学习** (lazy learning) 的主要代表。这种学习技术在训练时仅仅是把样本保存起来，训练时间开销为0，在接收到测试样本后再进行处理。与之对应的，在训练阶段就对样本进行学习处理的方法称为**急切学习**。

对于kNN 来说，**参数k** 十分重要：当k 取不同值时，分类结果会有显著不同；不同的**距离计算方式**找出的“近邻”则可能有显著差别。

令k = 1，我们就得到了**最近邻分类器**。给定测试样本x，若对应的最近邻样本为z，则最近邻分类器出错的概率就是x 与z 类别标记不同的概率：
$$
P(err)=1-\sum_{c \in Y} P(c|\mathbf{x}) P(c | \mathbf{z})
$$
假设对任意测试样本，总能在任意近的范围内找到式(1) 中的训练样本z，令$c^{*}=\arg\max_{c \in Y} P(c|\mathbf{x})$ 表示贝叶斯最优分类器的结果，则有：
$$
\begin{align}
P(error) & = 1 - \sum_{c \in Y} P(c | \mathbf{x}) P(c | \mathbf{z}) \\
 & \approx 1 - \sum_{c \in Y} P^2(c | \mathbf{x}) \\
 & \le 1 - P^2(c^* | \mathbf{x}) \\
 & = (1 + P(c^* | \mathbf{x})) (1 - P(c^*|\mathbf{x})) \\
 & \le 2 \times (1 - P(c^* | \mathbf{x}))
\end{align}
$$
上式的**结论**就是：最近邻分类器的泛化错误率不超过贝叶斯最优分类器的错误率的两倍。

## 低维嵌入

面前的讨论有一个重要的**假设**：任意测试样本x 附近足够下的距离范围内总能找到一个训练样本（密采样）。

但在高维情形下，将出现**数据样本悉数，距离计算困难**等问题，成为**维数灾难** (curse of dimensionality)。

缓解维数灾难有两种办法：**特征选择**和**降维** (dimension reduction)。

*降维*就是通过数学变换将原始高维属性空间转变为一个低维**子空间** (subspace)。在这个子空间中样本密度大幅提高，距离计算也较为容易。

进行降维背后的**假设**是：数据样本虽然是高维的，但是**与学习任务密切相关的知识其中某个低维分布**。

###MDS

若要求原始空间中样本之间的**距离**在低维空间中**得以保持**，即得到**多维缩放** (Multiple Dimensional Scaling，简称 MDS)。

假定m个样本在原始空间中的**距离矩阵**为$D \in R^{m \times m}$，dist~ij~ 为样本x~i~ 和x~j~ 之间的距离。我们希望获得样本在d' 维空间中的表示$Z \in R^{d' \times m},\ d' \le d$，且任意两个样本在d‘ 维空间中的（欧式）距离等于原始空间中的距离，即$||\mathbf{z}_i - \mathbf{z}_j|| = dist_{ij}$。

令$\mathbf{B} = \mathbf{Z}^T \mathbf{Z} \in R^{m \times m}$，则B 为降维后样本的内积矩阵，$b_{ij} = z_i^T z_j$，有
$$
\begin{align}
dist_{ij}^2 & = ||z_i||^2  + ||z_j||^2 - 2 z_i^T  z_j \\
 & = b_{ii} + b_{jj} - 2 b_{ij}
\end{align}
$$
假设降维后的样本Z 被**中心化**，即$\sum_{i=1}^m z_i = 0$，显然，矩阵B 的行与列之和均为0，即$\sum_{i=1}^m b_{ij} = \sum_{j = 1}^m b_{ij} = 0$ （将z~j~ 或者z~i~ 提取出来），易知：
$$
\begin{align}
\sum_{i=1}^m dist^2_{ij} & = \sum_{i=1}^m (b_{ii} + b_{jj} - 2 b_{ij}) \\
 & = \sum_{i=1}^m b_{ii} + \sum_{i=1}^m b{jj} -2 \sum_{i=1}^m b_{ij} \\
 & = tr(\mathbf{B}) + m \cdot b_{jj}
\end{align}
$$
类似的，有：
$$
\begin{align}
 & \sum_{j=1}^m dist^2_{ij} = tr(\mathbf{B}) + m b_{ii} \\
 & \sum_{i=1}^m \sum_{j=1}^m dist^2_{ij} = 2m\ tr(\mathbf{B})
\end{align}
$$
tr 为矩阵的迹，$tr(\mathbf{B}) = \sum_{i=1}^m ||z_i||^2$。令
$$
\begin{align}
dist^2_{i \cdot} & = \frac{1}{m} \sum_{j=1}^m dist^2_{ij} \\
dist^2_{\cdot j} & = \frac{1}{m} \sum_{i=1}^m dist^2_{ij} \\
dist^2_{\cdot \cdot} & = \frac{1}{m^2} \sum_{i=1}^m \sum_{j=1}^m dist^2_{ij}
\end{align}
$$
由以上这些式子可以得到：
$$
b_{ij} = -\frac{1}{2} (dist^2_{ij} - dist^2_{i \cdot} - dist^2_{\cdot j} + dist^2_{\cdot \cdot})
$$
说明**可以通过降维前后保持不变的距离矩阵D 来求解内积矩阵B**。

对矩阵B 进行**特征值分解**，$\mathbf{B} = \mathbf{V} \Lambda \mathbf{V}^T$。其中，$\Lambda = diag(\lambda_1, \lambda_2, \ldots, \lambda_d)$ 为特征值构成的对角阵，V 为特征向量矩阵。取前d^*^个不为零的特征值，构成对角阵$\Lambda_*=diag(\lambda_1, \ldots, \lambda_{d^*})$, V~*~ 为对应的特征向量矩阵，则Z 为
$$
\mathbf{Z} = \Lambda_* ^{1/2} \mathbf{V}_*^T \in R^{d^* \times m}
$$
在现实应用中为了有效降维，往往仅需降维后的距离与原始空间中的距离尽可能接近而不必严格相等。所以可取$d' \le d^* \ll d$ 个最大特征值。

![幕快照 2018-03-30 15.52.5](8/屏幕快照 2018-03-30 15.52.54.png)

###线性降维方法

而最简单的降维方法就是对原始高维空间做线性变换。设d 维空间中样本 $\mathbf{X} = (\mathbf{x}_1, \ldots, \mathbf{x}_m) \in R^{d \times m}$ ，变换后得到d' 维空间中的样本：
$$
\mathbf{Z} = \mathbf{W}^T \mathbf{X}
$$
$\mathbf{W} \in R^{d \times d'}$ 是变换矩阵。可以将之看作是d' 个d 维（竖）基向量，$\mathbf{z_i} = \mathbf{W}^T \mathbf{x}_i$ 是第i 个样本与这d' 个基向量**分别内积**而得到的d' 维属性向量。即z~i~ 是原属性向量x~i~ 在新坐标系W 中的**坐标向量**。显然，新空间中的属性是原空间的线性组合。

## 主成分分析

对于正交属性空间中的样本点，想用一个超平面对所有样本进行恰当的表达。该超平面有如下性质：

- **最近重构性**：样本点到超平面的距离足够近；
- **最大可分型**：样本点在这个超平面上的投影尽可能分开。

假定数据样本已去中心化，即$\sum_i \mathbf{x}_i = 0$；投影变换后的坐标系为$\mathbf{W} = \{\mathbf{\omega}_1, \ldots, \mathbf{\omega}_d \}$, 其中，w~i~ 是**标准正交基向量**，$||\omega_i || = 1,\ \omega_i^T \omega_j = 0$；将维度降低到d'，样本点x~i~ 的投影为$\mathbf{z}_i = (z_{i1}; \ldots; z_{id'}),\ z_{ij} = \omega_j^T \mathbf{x}_i$ 是x~i~ 在低维坐标系下第j 维的坐标；而重构的$\mathbf{\hat x}_i =\sum_{j=1}^{d'} z_ij \omega_j$。

考虑整个训练集，原样本点x~i~ 和基于投影重构的样本点$\mathbf{\hat x}_i$ 之间的**距离**为：
$$
\begin{align}
\sum_{i = 1}^m \begin{Vmatrix} \sum_{j = 1}^{d'} z_{ij} \omega_j - \mathbf{x}_i \end{Vmatrix}^2_2
 & = \sum_{i=1}^m \mathbf{z}^T_i \mathbf{z}_i - 2 \sum_{i=1}^m \mathbf{z}_i^T \mathbf{W}^T \mathbf{x}_i + const \\
 & \propto -tr \begin{pmatrix} \mathbf{W}^T(\sum_{i=1}^m \mathbf{x}_i \mathbf{x}_i^T) \mathbf{W} \end{pmatrix}
\end{align}
$$
依据最近重构性，上式应该被最小化。考虑到$\sum_{i=1}^m \mathbf{x}_i \mathbf{x}_i^T$ 是协方差矩阵，有
$$
\min_{\mathbf{W}} \ -tr(\mathbf{W}^T \mathbf{X} \mathbf{X}^T \mathbf{W}) \ \ s.t. \mathbf{W}^T \mathbf{W} = \mathbf{I}
$$
这就是PCA 的优化目标。

从最大可分行出发，能得到同样的结果。样本点x~i~ 在子空间的投影为$\mathbf{W}^T \mathbf{x}_i$，若所有样本点的投影能尽可能分开，则应该使投影后的样本点的**方差最大化**。而投影后的样本点的方差为$\sum_i \mathbf{W}^T \mathbf{x}_i \mathbf{x}_i^T \mathbf{W}$，优化目标就可以写为：
$$
\max_{\mathbf{W}} \ tr(\mathbf{W}^T \mathbf{X} \mathbf{X}^T \mathbf{W}) \ \ s.t. \mathbf{W}^T \mathbf{W} = \mathbf{I}
$$
显然，式(23) 和式(22) 等价。对这两个式子使用*拉格朗日乘子法*可得
$$
\mathbf{X} \mathbf{X}^T \mathbf{W} = \lambda \mathbf{W}
$$
只需要对协方差矩阵$\mathbf{X} \mathbf{X}^T$ 进行特征值分解（实践中常常对$\mathbf{X}$ 进行SVD 分解），取降序排序后的前d' 个特征值对应的**特征向量**构成$\mathbf{W} = (\omega_1, \ldots, \omega_{d'})$，就是PCA 的解。

![幕快照 2018-03-30 19.40.2](8/屏幕快照 2018-03-30 19.40.23.png)

PCA 也可以看作是逐一选取**方差最大**方向：对协方差矩阵进行特征值分解，取最大特征值对应的特征向量w~1~；再对$\sum_i \mathbf{x}_i \mathbf{x}^T_i - \lambda_1 \omega_1 \omega_1^T$ 进行特征值分解，去最大特征值对应的特征向量w~2~；如此循环。该方法与前述算法等价。

d' 的确定：

- 用户指定；
- 通过在d' 值不同的低维空间中对k 近邻分类器进行**交叉验证**来选取；
- 设置**重构阈值**：$\frac{\sum_{i=1}^{d'} \lambda_i}{\sum_{i=1}^d \lambda_i} \ge t$。

PCA 仅仅需要保留W 和样本的**均值向量**即可通过向量加减和矩阵乘法将新样本投影到低维空间中。但最小的d - d' 个特征值的特征向量被**舍弃**了。**影响**：

- 舍弃这部分信息能使样本的**采样密度**增大；
- 当数据受到噪声影响时，最小的特征值对应的特征向量往往与噪声有关。舍弃能达到**去噪**的效果。