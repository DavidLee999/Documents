# 降维

## k 近邻学习

k 近邻 (k-Nearest Neighbor，简称kNN) 学习的**工作机制**：给定测试样本，基于某种距离度量找出训练集中与其最靠近的k 个训练样本，然后基于着k 个邻居的信息来进行预测。在分类中常使用投票法；在回归任务中可使用平均法。也可以将距离考虑在内，使用加权平均或者加权投票。

kNN 其实是**懒惰学习** (lazy learning) 的主要代表。这种学习技术在训练时仅仅是把样本保存起来，训练时间开销为0，在接收到测试样本后再进行处理。与之对应的，在训练阶段就对样本进行学习处理的方法称为**急切学习**。

对于kNN 来说，**参数k** 十分重要：当k 取不同值时，分类结果会有显著不同；不同的**距离计算方式**找出的“近邻”则可能有显著差别。

令k = 1，我们就得到了**最近邻分类器**。给定测试样本x，若对应的最近邻样本为z，则最近邻分类器出错的概率就是x 与z 类别标记不同的概率：
$$
P(err)=1-\sum_{c \in Y} P(c|\mathbf{x}) P(c | \mathbf{z})
$$
假设对任意测试样本，总能在任意近的范围内找到式(1) 中的训练样本z，令$c^{*}=\arg\max_{c \in Y} P(c|\mathbf{x})$ 表示贝叶斯最优分类器的结果，则有：
$$
\begin{align}
P(error) & = 1 - \sum_{c \in Y} P(c | \mathbf{x}) P(c | \mathbf{z}) \\
 & \approx 1 - \sum_{c \in Y} P^2(c | \mathbf{x}) \\
 & \le 1 - P^2(c^* | \mathbf{x}) \\
 & = (1 + P(c^* | \mathbf{x})) (1 - P(c^*|\mathbf{x})) \\
 & \le 2 \times (1 - P(c^* | \mathbf{x}))
\end{align}
$$
上式的**结论**就是：最近邻分类器的泛化错误率不超过贝叶斯最优分类器的错误率的两倍。

## 低维嵌入

面前的讨论有一个重要的**假设**：任意测试样本x 附近足够下的距离范围内总能找到一个训练样本（密采样）。

但在高维情形下，将出现**数据样本悉数，距离计算困难**等问题，成为**维数灾难** (curse of dimensionality)。

缓解维数灾难有两种办法：**特征选择**和**降维** (dimension reduction)。

*降维*就是通过数学变换将原始高维属性空间转变为一个低维**子空间** (subspace)。在这个子空间中样本密度大幅提高，距离计算也较为容易。

进行降维背后的**假设**是：数据样本虽然是高维的，但是**与学习任务密切相关的知识其中某个低维分布**。

###MDS

若要求原始空间中样本之间的**距离**在低维空间中**得以保持**，即得到**多维缩放** (Multiple Dimensional Scaling，简称 MDS)。

假定m个样本在原始空间中的**距离矩阵**为$D \in R^{m \times m}$，dist~ij~ 为样本x~i~ 和x~j~ 之间的距离。我们希望获得样本在d' 维空间中的表示$Z \in R^{d' \times m},\ d' \le d$，且任意两个样本在d‘ 维空间中的（欧式）距离等于原始空间中的距离，即$||\mathbf{z}_i - \mathbf{z}_j|| = dist_{ij}$。

令$\mathbf{B} = \mathbf{Z}^T \mathbf{Z} \in R^{m \times m}$，则B 为降维后样本的内积矩阵，$b_{ij} = z_i^T z_j$，有
$$
\begin{align}
dist_{ij}^2 & = ||z_i||^2  + ||z_j||^2 - 2 z_i^T  z_j \\
 & = b_{ii} + b_{jj} - 2 b_{ij}
\end{align}
$$
假设降维后的样本Z 被**中心化**，即$\sum_{i=1}^m z_i = 0$，显然，矩阵B 的行与列之和均为0，即$\sum_{i=1}^m b_{ij} = \sum_{j = 1}^m b_{ij} = 0$ （将z~j~ 或者z~i~ 提取出来），易知：
$$
\begin{align}
\sum_{i=1}^m dist^2_{ij} & = \sum_{i=1}^m (b_{ii} + b_{jj} - 2 b_{ij}) \\
 & = \sum_{i=1}^m b_{ii} + \sum_{i=1}^m b{jj} -2 \sum_{i=1}^m b_{ij} \\
 & = tr(\mathbf{B}) + m \cdot b_{jj}
\end{align}
$$
类似的，有：
$$
\begin{align}
 & \sum_{j=1}^m dist^2_{ij} = tr(\mathbf{B}) + m b_{ii} \\
 & \sum_{i=1}^m \sum_{j=1}^m dist^2_{ij} = 2m\ tr(\mathbf{B})
\end{align}
$$
tr 为矩阵的迹，$tr(\mathbf{B}) = \sum_{i=1}^m ||z_i||^2$。令
$$
\begin{align}
dist^2_{i \cdot} & = \frac{1}{m} \sum_{j=1}^m dist^2_{ij} \\
dist^2_{\cdot j} & = \frac{1}{m} \sum_{i=1}^m dist^2_{ij} \\
dist^2_{\cdot \cdot} & = \frac{1}{m^2} \sum_{i=1}^m \sum_{j=1}^m dist^2_{ij}
\end{align}
$$
由以上这些式子可以得到：
$$
b_{ij} = -\frac{1}{2} (dist^2_{ij} - dist^2_{i \cdot} - dist^2_{\cdot j} + dist^2_{\cdot \cdot})
$$
说明**可以通过降维前后保持不变的距离矩阵D 来求解内积矩阵B**。

对矩阵B 进行**特征值分解**，$\mathbf{B} = \mathbf{V} \Lambda \mathbf{V}^T$。其中，$\Lambda = diag(\lambda_1, \lambda_2, \ldots, \lambda_d)$ 为特征值构成的对角阵，V 为特征向量矩阵。取前d^*^个不为零的特征值，构成对角阵$\Lambda_*=diag(\lambda_1, \ldots, \lambda_{d^*})$, V~*~ 为对应的特征向量矩阵，则Z 为
$$
\mathbf{Z} = \Lambda_* ^{1/2} \mathbf{V}_*^T \in R^{d^* \times m}
$$
在现实应用中为了有效降维，往往仅需降维后的距离与原始空间中的距离尽可能接近而不必严格相等。所以可取$d' \le d^* \ll d$ 个最大特征值。

![幕快照 2018-03-30 15.52.5](../../../../../Downloads/屏幕快照 2018-03-30 15.52.54.png)

###线性降维方法

而最简单的降维方法就是对原始高维空间做线性变换。设d 维空间中样本 $\mathbf{X} = (\mathbf{x}_1, \ldots, \mathbf{x}_m) \in R^{d \times m}$ ，变换后得到d' 维空间中的样本：
$$
\mathbf{Z} = \mathbf{W}^T \mathbf{X}
$$
$\mathbf{W} \in R^{d \times d'}$ 是变换矩阵。可以将之看作是d' 个d 维（竖）基向量，$\mathbf{z_i} = \mathbf{W}^T \mathbf{x}_i$ 是第i 个样本与这d' 个基向量**分别内积**而得到的d' 维属性向量。即z~i~ 是原属性向量x~i~ 在新坐标系W 中的**坐标向量**。显然，新空间中的属性是原空间的线性组合。

## 主成分分析

